{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Extraction with Snorkel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matheus Schmitz\n",
    "<br><a href=\"https://www.linkedin.com/in/matheusschmitz/\">LinkedIn</a></br>\n",
    "<br><a href=\"https://matheus-schmitz.github.io/\">Github Portfolio</a></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State-of-the-art extraction techniques require massive labeled training set but it is costly to obtain. To overcome this problem, Snorkel helps rapidly create training sets using the new data programming paradigm. To start, developers focus on writing a set of labeling functions, which are just scripts that programmatically label data. The resulting labels are noisy, but Snorkel uses a generative model to learn how to use those labeling functions to label more data. The new labeled data now can be used to train high-quality end models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepara Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all actor briographies crawled from IMDB\n",
    "import pandas as pd\n",
    "import os\n",
    "if not os.path.isfile('Matheus_Schmitz_hw05_all.tsv'):\n",
    "    # Read the example dataset\n",
    "    df1 = pd.read_csv('cast_bios.tsv', sep='\\t', header=None)\n",
    "    print(f'df1.shape: {df1.shape}')\n",
    "    \n",
    "    # Read my dataset from hw02\n",
    "    df2 = pd.read_csv('Matheus_Schmitz_hw02_bios.csv', header=None)\n",
    "    df2.to_csv('Matheus_Schmitz_hw05_bio.tsv', header=False, index=False, sep='\\t')\n",
    "    print(f'df2.shape: {df2.shape}')\n",
    "    \n",
    "    # Merge the datasets\n",
    "    df_merged = pd.concat([df1, df2])\n",
    "    print(f'df_merged.shape: {df_merged.shape}')\n",
    "    \n",
    "    # Deduplicate\n",
    "    df_merged.drop_duplicates(inplace=True)\n",
    "    print(f'deduplication: {df_merged.shape}')\n",
    "    \n",
    "    # Save as CSV and TSV\n",
    "    df_merged.to_csv('Matheus_Schmitz_hw05_all.csv', header=False, index=False)\n",
    "    df_merged.to_csv('Matheus_Schmitz_hw05_all.tsv', header=False, index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Snorkel Environment\n",
    "\n",
    "Lets install the packages we will use. Through my testing, Snorkel v0.7 works the best with Python 3.6 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new Python 3.6 environment.\n",
    "\n",
    "!conda create -n py36 python=3.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -L \"https://github.com/snorkel-team/snorkel/archive/v0.7.0-beta.tar.gz\" -o snorkel_v0_7_0.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's uncompress the package and install Snorkel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xvzf snorkel_v0_7_0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install snorkel-0.7.0-beta/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Development Set\n",
    "\n",
    "We need to preprocess our documents using `Snorkel` utilities, parsing them into a simple hierarchy of component parts of our input data, which we refer as _contexts_. We'll also create _candidates_ out of these contexts, which are the objects we want to classify, in this case, possible mentions of schools and colleges that the cast have attended. Finally, we'll load some gold labels for evaluation.\n",
    "\n",
    "All of this preprocessed input data is saved to a database. In Snorkel, if no database is specified, then a SQLite database at `./snorkel.db` is created by default -- so no setup is needed here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Packages\n",
    "import numpy as np, os\n",
    "from pathlib import Path\n",
    "import re\n",
    "from snorkel import SnorkelSession\n",
    "from snorkel.parser import TSVDocPreprocessor, CorpusParser, CSVPathsPreprocessor\n",
    "from snorkel.parser.spacy_parser import Spacy\n",
    "from snorkel.models import Document, Sentence, candidate_subclass\n",
    "from snorkel.viewer import SentenceNgramViewer\n",
    "from snorkel.annotations import LabelAnnotator, load_gold_labels\n",
    "\n",
    "# TODO: SET LOCATION WHERE YOU STORE YOUR HW5 FILES\n",
    "if 'HW_DIR' not in os.environ:\n",
    "    HW_DIR = Path(\".\")\n",
    "else:\n",
    "    HW_DIR = Path(os.environ['HW_DIR'])\n",
    "    assert HW_DIR.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing a `SnorkelSession`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "session = SnorkelSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Corpus\n",
    "\n",
    "Next, we load and pre-process the corpus of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using only hw2 dataset\n",
    "doc_preprocessor = TSVDocPreprocessor(HW_DIR / 'Matheus_Schmitz_hw05_bio.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a `CorpusParser`\n",
    "\n",
    "We'll use [Spacy](https://spacy.io/), an NLP preprocessing tool, to split our documents into sentences and tokens, and provide named entity annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this to download spacy model\n",
    "#!python -m spacy download [model_name] (e.g. en_core_web_lg)\n",
    "\n",
    "corpus_parser = CorpusParser(parser=Spacy())\n",
    "%time corpus_parser.apply(doc_preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use simple database queries (written in the syntax of [SQLAlchemy](http://www.sqlalchemy.org/), which Snorkel uses) to check how many documents and sentences were parsed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents: 982\n",
      "Sentences: 4519\n"
     ]
    }
   ],
   "source": [
    "print(\"Documents:\", session.query(Document).count())\n",
    "print(\"Sentences:\", session.query(Sentence).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Candidates\n",
    "\n",
    "The next step is to extract _candidates_ from our corpus. A `Candidate` in Snorkel is an object for which we want to make a prediction. In this case, the candidates are pairs of performances and directors mentioned in sentences.\n",
    "\n",
    "The [Spacy](https://spacy.io/) parser we used performs _named entity recognition_ for us. Next, we'll split up the documents into train and development splits; and collect the associated sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing a Director Name Matching Function\n",
    "\n",
    "Our **simple** name matcher makes use of the fact that the names of the directors are mentions of person-type named entities in the documents. `Fonduer` provides a list of built-in matchers that can be used in many information extraction tasks. We will use `PersonMatcher` to extract director names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from snorkel.matchers import PersonMatcher, OrganizationMatcher\n",
    "from snorkel.matchers import RegexMatchEach, LambdaFunctionMatcher\n",
    "\n",
    "director_matcher = PersonMatcher(longest_match_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing a Performance (Movie) Matching Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.matchers import RegexMatchEach, LambdaFunctionMatcher\n",
    "\n",
    "parethesis_year = re.compile(r\"[A-Z][a-z]*\\s\\(\\d{4}\\)\")\n",
    "single_quotes_regex = re.compile(r\"'[^']+'\")\n",
    "double_quotes_regex = re.compile(r'\"[^\\']+\"')\n",
    "directed_by_regex = re.compile('directed by')\n",
    "def has_performance(mention):\n",
    "    if 'direct' in mention.sentence.text:\n",
    "        ner_tag = mention.get_attrib_span('ner_tags')\n",
    "        if \" O \" in ner_tag and \"ORG\" not in ner_tag and \"PERSON\" not in ner_tag:\n",
    "            performance_string = mention.get_span()\n",
    "            m1 = parethesis_year.findall(performance_string) # (1234)\n",
    "            m2 = double_quotes_regex.findall(performance_string) # \"Movie Name\"\n",
    "            m3 = directed_by_regex.findall(performance_string) # directed by\n",
    "            m4 = single_quotes_regex.findall(performance_string) # 'Movie Name'\n",
    "            matches = m1 + m2 + m3\n",
    "            if len(matches) > 0:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "      \n",
    "performance_matcher = LambdaFunctionMatcher(func=has_performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that normally each `director` name will contain at least two words (first name, last name). Considering\n",
    "additional middle names, we expect a maximum of four words per name.\n",
    "\n",
    "Similarly, we assume the `performance` name to be a `span` of one to seven words.\n",
    "\n",
    "We use the default `Ngrams` class provided by `Fonduer` to define these properties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.candidates import Ngrams\n",
    "performance_ngrams = Ngrams(n_max=7)\n",
    "director_ngrams = Ngrams(n_max=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a candidate that is composed of a `performance` and a `director` mention as we defined above. We name this candidate `performance_director`. And we will extract all "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.candidates import Ngrams, CandidateExtractor\n",
    "\n",
    "performance_with_director = candidate_subclass('performance_director', ['performance', 'director'])\n",
    "ngrams = Ngrams(n_max=7)\n",
    "cand_extractor = CandidateExtractor(performance_with_director, [performance_ngrams, director_ngrams], [performance_matcher, director_matcher])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Development Set\n",
    "\n",
    "We create our development set by generating a `dev_ids.csv` file, which has one column `id` and contains 50 random biography URLs. You can choose any subset of 50 biographies that have `performance` and `director`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dev documents: 50\n",
      "Number of dev sents: 756\n",
      "Number of train sents: 3763\n"
     ]
    }
   ],
   "source": [
    "docs = session.query(Document).order_by(Document.name).all()\n",
    "import pandas as pd\n",
    "\n",
    "docs = session.query(Document).order_by(Document.name).all()\n",
    "ld = len(docs)\n",
    "\n",
    "gold_data = pd.read_csv(\"dev_ids.csv\")\n",
    "\n",
    "dev_docs = gold_data[\"id\"].values.tolist()\n",
    "\n",
    "print(f\"Number of dev documents: {len(dev_docs)}\")\n",
    "\n",
    "train_sents = set()\n",
    "dev_sents   = set()\n",
    "\n",
    "for doc in docs:\n",
    "    sents = [s for s in doc.sentences]\n",
    "    if doc.name in dev_docs:\n",
    "        dev_sents.update(sents)\n",
    "    else:\n",
    "        train_sents.update(sents)\n",
    "        \n",
    "print(\"Number of dev sents:\", len(dev_sents))\n",
    "print(\"Number of train sents:\", len(train_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll apply the candidate extractor to the two sets of sentences. The results will be persisted in the database backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "Number of candidates: 87\n",
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "Number of candidates: 212\n",
      "Wall time: 9.64 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for i, sents in enumerate([train_sents, dev_sents]):\n",
    "    cand_extractor.apply(sents, split=i)\n",
    "    print(\"Number of candidates:\", session.query(performance_with_director).filter(performance_with_director.split == i).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Documents in Development Set\n",
    "\n",
    "Using `SentenceNgramViewer` to label each mention. You can click the green button to mark the candidate as correct, red button to mark as incorrect. Your labeling result is automatically stored in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number unlabeled: 212\n"
     ]
    }
   ],
   "source": [
    "from snorkel.models import GoldLabel, GoldLabelKey\n",
    "\n",
    "def get_gold_labels(session: SnorkelSession, annotator_name: str=\"gold\"):\n",
    "    # define relationship in case it is not defined\n",
    "    ak = session.query(GoldLabelKey).filter(GoldLabelKey.name == annotator_name).first()\n",
    "    return session.query(GoldLabel).filter(GoldLabel.key == ak).all()\n",
    "\n",
    "gold_labels = get_gold_labels(session)\n",
    "labeled_sents = {lbl.candidate.performance.sentence.id for lbl in gold_labels}\n",
    "unlabeled = [\n",
    "    x for x in session.query(performance_with_director).filter(performance_with_director.split == 1).all() \n",
    "    if x.performance.sentence.id not in labeled_sents\n",
    "]\n",
    "print(\"Number unlabeled:\", len(unlabeled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SentenceNgramViewer` only show candidates that are matched by the matchers. Therefore, the annotation is under an assumption that the matchers work perfectly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix \"SentenceNgramViewer\" text instead of a UI component.\n",
    "#!jupyter nbextension enable --py --sys-prefix widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "require.undef('viewer');\n",
       "\n",
       "// NOTE: all elements should be selected using this.$el.find to avoid collisions with other Viewers\n",
       "\n",
       "define('viewer', [\"@jupyter-widgets/base\"], function(widgets) {\n",
       "    var ViewerView = widgets.DOMWidgetView.extend({\n",
       "        render: function() {\n",
       "            this.cids   = this.model.get('cids');\n",
       "            this.nPages = this.cids.length;\n",
       "            this.pid  = 0;\n",
       "            this.cxid = 0;\n",
       "            this.cid  = 0;\n",
       "\n",
       "            // Insert the html payload\n",
       "            this.$el.append(this.model.get('html'));\n",
       "\n",
       "            // Initialize all labels from previous sessions\n",
       "            this.labels = this.deserializeDict(this.model.get('_labels_serialized'));\n",
       "            for (var i=0; i < this.nPages; i++) {\n",
       "                this.pid = i;\n",
       "                for (var j=0; j < this.cids[i].length; j++) {\n",
       "                    this.cxid = j;\n",
       "                    for (var k=0; k < this.cids[i][j].length; k++) {\n",
       "                        this.cid = k;\n",
       "                        if (this.cids[i][j][k] in this.labels) {\n",
       "                            this.markCurrentCandidate(false);\n",
       "                        }\n",
       "                    }\n",
       "                }\n",
       "            }\n",
       "            this.pid  = 0;\n",
       "            this.cxid = 0;\n",
       "            this.cid  = 0;\n",
       "\n",
       "            // Enable button functionality for navigation\n",
       "            var that = this;\n",
       "            this.$el.find(\"#next-cand\").click(function() {\n",
       "                that.switchCandidate(1);\n",
       "            });\n",
       "            this.$el.find(\"#prev-cand\").click(function() {\n",
       "                that.switchCandidate(-1);\n",
       "            });\n",
       "            this.$el.find(\"#next-context\").click(function() {\n",
       "                that.switchContext(1);\n",
       "            });\n",
       "            this.$el.find(\"#prev-context\").click(function() {\n",
       "                that.switchContext(-1);\n",
       "            });\n",
       "            this.$el.find(\"#next-page\").click(function() {\n",
       "                that.switchPage(1);\n",
       "            });\n",
       "            this.$el.find(\"#prev-page\").click(function() {\n",
       "                that.switchPage(-1);\n",
       "            });\n",
       "            this.$el.find(\"#label-true\").click(function() {\n",
       "                that.labelCandidate(true, true);\n",
       "            });\n",
       "            this.$el.find(\"#label-false\").click(function() {\n",
       "                that.labelCandidate(false, true);\n",
       "            });\n",
       "\n",
       "            // Arrow key functionality\n",
       "            this.$el.keydown(function(e) {\n",
       "                switch(e.which) {\n",
       "                    case 74: // j\n",
       "                    that.switchCandidate(-1);\n",
       "                    break;\n",
       "\n",
       "                    case 73: // i\n",
       "                    that.switchPage(-1);\n",
       "                    break;\n",
       "\n",
       "                    case 76: // l\n",
       "                    that.switchCandidate(1);\n",
       "                    break;\n",
       "\n",
       "                    case 75: // k\n",
       "                    that.switchPage(1);\n",
       "                    break;\n",
       "\n",
       "                    case 84: // t\n",
       "                    that.labelCandidate(true, true);\n",
       "                    break;\n",
       "\n",
       "                    case 70: // f\n",
       "                    that.labelCandidate(false, true);\n",
       "                    break;\n",
       "                }\n",
       "            });\n",
       "\n",
       "            // Show the first page and highlight the first candidate\n",
       "            this.$el.find(\"#viewer-page-0\").show();\n",
       "            this.switchCandidate(0);\n",
       "        },\n",
       "\n",
       "        // Get candidate selector for currently selected candidate, escaping id properly\n",
       "        getCandidate: function() {\n",
       "            return this.$el.find(\".\"+this.cids[this.pid][this.cxid][this.cid]);\n",
       "        },  \n",
       "\n",
       "        // Color the candidate correctly according to registered label, as well as set highlighting\n",
       "        markCurrentCandidate: function(highlight) {\n",
       "            var cid  = this.cids[this.pid][this.cxid][this.cid];\n",
       "            var tags = this.$el.find(\".\"+cid);\n",
       "\n",
       "            // Clear color classes\n",
       "            tags.removeClass(\"candidate-h\");\n",
       "            tags.removeClass(\"true-candidate\");\n",
       "            tags.removeClass(\"true-candidate-h\");\n",
       "            tags.removeClass(\"false-candidate\");\n",
       "            tags.removeClass(\"false-candidate-h\");\n",
       "            tags.removeClass(\"highlighted\");\n",
       "\n",
       "            if (highlight) {\n",
       "                if (cid in this.labels) {\n",
       "                    tags.addClass(String(this.labels[cid]) + \"-candidate-h\");\n",
       "                } else {\n",
       "                    tags.addClass(\"candidate-h\");\n",
       "                }\n",
       "            \n",
       "            // If un-highlighting, leave with first non-null coloring\n",
       "            } else {\n",
       "                var that = this;\n",
       "                tags.each(function() {\n",
       "                    var cids = $(this).attr('class').split(/\\s+/).map(function(item) {\n",
       "                        return parseInt(item);\n",
       "                    });\n",
       "                    cids.sort();\n",
       "                    for (var i in cids) {\n",
       "                        if (cids[i] in that.labels) {\n",
       "                            var label = that.labels[cids[i]];\n",
       "                            $(this).addClass(String(label) + \"-candidate\");\n",
       "                            $(this).removeClass(String(!label) + \"-candidate\");\n",
       "                            break;\n",
       "                        }\n",
       "                    }\n",
       "                });\n",
       "            }\n",
       "\n",
       "            // Extra highlighting css\n",
       "            if (highlight) {\n",
       "                tags.addClass(\"highlighted\");\n",
       "            }\n",
       "\n",
       "            // Classes for showing direction of relation\n",
       "            if (highlight) {\n",
       "                this.$el.find(\".\"+cid+\"-0\").addClass(\"left-candidate\");\n",
       "                this.$el.find(\".\"+cid+\"-1\").addClass(\"right-candidate\");\n",
       "            } else {\n",
       "                this.$el.find(\".\"+cid+\"-0\").removeClass(\"left-candidate\");\n",
       "                this.$el.find(\".\"+cid+\"-1\").removeClass(\"right-candidate\");\n",
       "            }\n",
       "        },\n",
       "\n",
       "        // Cycle through candidates and highlight, by increment inc\n",
       "        switchCandidate: function(inc) {\n",
       "            var N = this.cids[this.pid].length\n",
       "            var M = this.cids[this.pid][this.cxid].length;\n",
       "            if (N == 0 || M == 0) { return false; }\n",
       "\n",
       "            // Clear highlighting from previous candidate\n",
       "            if (inc != 0) {\n",
       "                this.markCurrentCandidate(false);\n",
       "\n",
       "                // Increment the cid counter\n",
       "\n",
       "                // Move to next context\n",
       "                if (this.cid + inc >= M) {\n",
       "                    while (this.cid + inc >= M) {\n",
       "                        \n",
       "                        // At last context on page, halt\n",
       "                        if (this.cxid == N - 1) {\n",
       "                            this.cid = M - 1;\n",
       "                            inc = 0;\n",
       "                            break;\n",
       "                        \n",
       "                        // Increment to next context\n",
       "                        } else {\n",
       "                            inc -= M - this.cid;\n",
       "                            this.cxid += 1;\n",
       "                            M = this.cids[this.pid][this.cxid].length;\n",
       "                            this.cid = 0;\n",
       "                        }\n",
       "                    }\n",
       "\n",
       "                // Move to previous context\n",
       "                } else if (this.cid + inc < 0) {\n",
       "                    while (this.cid + inc < 0) {\n",
       "                        \n",
       "                        // At first context on page, halt\n",
       "                        if (this.cxid == 0) {\n",
       "                            this.cid = 0;\n",
       "                            inc = 0;\n",
       "                            break;\n",
       "                        \n",
       "                        // Increment to previous context\n",
       "                        } else {\n",
       "                            inc += this.cid + 1;\n",
       "                            this.cxid -= 1;\n",
       "                            M = this.cids[this.pid][this.cxid].length;\n",
       "                            this.cid = M - 1;\n",
       "                        }\n",
       "                    }\n",
       "                }\n",
       "\n",
       "                // Move within current context\n",
       "                this.cid += inc;\n",
       "            }\n",
       "            this.markCurrentCandidate(true);\n",
       "\n",
       "            // Push this new cid to the model\n",
       "            this.model.set('_selected_cid', this.cids[this.pid][this.cxid][this.cid]);\n",
       "            this.touch();\n",
       "        },\n",
       "\n",
       "        // Switch through contexts\n",
       "        switchContext: function(inc) {\n",
       "            this.markCurrentCandidate(false);\n",
       "\n",
       "            // Iterate context on this page\n",
       "            var M = this.cids[this.pid].length;\n",
       "            if (this.cxid + inc < 0) {\n",
       "                this.cxid = 0;\n",
       "            } else if (this.cxid + inc >= M) {\n",
       "                this.cxid = M - 1;\n",
       "            } else {\n",
       "                this.cxid += inc;\n",
       "            }\n",
       "\n",
       "            // Reset cid and set to first candidate\n",
       "            this.cid = 0;\n",
       "            this.switchCandidate(0);\n",
       "        },\n",
       "\n",
       "        // Switch through pages\n",
       "        switchPage: function(inc) {\n",
       "            this.markCurrentCandidate(false);\n",
       "            this.$el.find(\".viewer-page\").hide();\n",
       "            if (this.pid + inc < 0) {\n",
       "                this.pid = 0;\n",
       "            } else if (this.pid + inc > this.nPages - 1) {\n",
       "                this.pid = this.nPages - 1;\n",
       "            } else {\n",
       "                this.pid += inc;\n",
       "            }\n",
       "            this.$el.find(\"#viewer-page-\"+this.pid).show();\n",
       "\n",
       "            // Show pagination\n",
       "            this.$el.find(\"#page\").html(this.pid);\n",
       "\n",
       "            // Reset cid and set to first candidate\n",
       "            this.cid = 0;\n",
       "            this.cxid = 0;\n",
       "            this.switchCandidate(0);\n",
       "        },\n",
       "\n",
       "        // Label currently-selected candidate\n",
       "        labelCandidate: function(label, highlighted) {\n",
       "            var c    = this.getCandidate();\n",
       "            var cid  = this.cids[this.pid][this.cxid][this.cid];\n",
       "            var cl   = String(label) + \"-candidate\";\n",
       "            var clh  = String(label) + \"-candidate-h\";\n",
       "            var cln  = String(!label) + \"-candidate\";\n",
       "            var clnh = String(!label) + \"-candidate-h\";\n",
       "\n",
       "            // Toggle label highlighting\n",
       "            if (c.hasClass(cl) || c.hasClass(clh)) {\n",
       "                c.removeClass(cl);\n",
       "                c.removeClass(clh);\n",
       "                if (highlighted) {\n",
       "                    c.addClass(\"candidate-h\");\n",
       "                }\n",
       "                this.labels[cid] = null;\n",
       "                this.send({event: 'delete_label', cid: cid});\n",
       "            } else {\n",
       "                c.removeClass(cln);\n",
       "                c.removeClass(clnh);\n",
       "                if (highlighted) {\n",
       "                    c.addClass(clh);\n",
       "                } else {\n",
       "                    c.addClass(cl);\n",
       "                }\n",
       "                this.labels[cid] = label;\n",
       "                this.send({event: 'set_label', cid: cid, value: label});\n",
       "            }\n",
       "\n",
       "            // Set the label and pass back to the model\n",
       "            this.model.set('_labels_serialized', this.serializeDict(this.labels));\n",
       "            this.touch();\n",
       "        },\n",
       "\n",
       "        // Serialization of hash maps, because traitlets Dict doesn't seem to work...\n",
       "        serializeDict: function(d) {\n",
       "            var s = [];\n",
       "            for (var key in d) {\n",
       "                s.push(key+\"~~\"+d[key]);\n",
       "            }\n",
       "            return s.join();\n",
       "        },\n",
       "\n",
       "        // Deserialization of hash maps\n",
       "        deserializeDict: function(s) {\n",
       "            var d = {};\n",
       "            var entries = s.split(/,/);\n",
       "            var kv;\n",
       "            for (var i in entries) {\n",
       "                kv = entries[i].split(/~~/);\n",
       "                if (kv[1] == \"true\") {\n",
       "                    d[kv[0]] = true;\n",
       "                } else if (kv[1] == \"false\") {\n",
       "                    d[kv[0]] = false;\n",
       "                }\n",
       "            }\n",
       "            return d;\n",
       "        },\n",
       "    });\n",
       "\n",
       "    return {\n",
       "        ViewerView: ViewerView\n",
       "    };\n",
       "});\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "359aba4a1eef419da181efad7426587a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SentenceNgramViewer(cids=[[[123], [175, 176, 177, 181], [74, 75, 76, 77, 86, 87, 88, 89, 111, 112, 113, 114, 1â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SentenceNgramViewer(unlabeled, session, annotator_name=\"gold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save labeled data as CSV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_gold_labels(session: SnorkelSession, annotator_name: str=\"gold\", split: int=None):\n",
    "    ''' Extract pairwise gold labels and store in a file. '''\n",
    "    gold_labels = get_gold_labels(session, annotator_name)\n",
    "\n",
    "    results = []\n",
    "    for gold_label in gold_labels:\n",
    "        rel = gold_label.candidate\n",
    "        if split is not None and rel.split != split:\n",
    "            continue\n",
    "\n",
    "        results.append({\n",
    "            \"id\": rel.performance.sentence.document.name,\n",
    "            \"performance\": rel.performance.get_span(),\n",
    "            \"director\": rel.director.get_span(),\n",
    "            \"value\": gold_label.value\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "#gold_labels = extract_gold_labels(session, split=1)\n",
    "#gold_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame(gold_labels).to_csv(\"Matheus_Schmitz_hw05_gold.dev.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from snorkel.models import StableLabel\n",
    "from snorkel.db_helpers import reload_annotator_labels\n",
    "\n",
    "def save_gold_labels(session: SnorkelSession, annotator_name: str=\"gold\", split: int=None, output_file=\"saved_gold.json\"):\n",
    "    ''' Extract pairwise gold labels and store in a file. '''\n",
    "    gold_labels = get_gold_labels(session, annotator_name)\n",
    "\n",
    "    results = []\n",
    "    for gold_label in gold_labels:\n",
    "        rel = gold_label.candidate\n",
    "        if split is not None and rel.split != split:\n",
    "            continue\n",
    "\n",
    "        results.append({\n",
    "            \"performance\": rel.performance.stable_id,\n",
    "            \"director\": rel.director.stable_id,\n",
    "            \"value\": gold_label.value\n",
    "        })\n",
    "\n",
    "    with open(str(output_file), \"w\") as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "        \n",
    "#save_gold_labels(session, \"gold\", split=1, output_file=\"saved_gold.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnnotatorLabels created: 212\n"
     ]
    }
   ],
   "source": [
    "def reload_external_labels(session: SnorkelSession, input_file, annotator_name: str=\"gold\", split: int=None):\n",
    "    performance_with_director = candidate_subclass('performance_director', ['performance', 'director'])\n",
    "    with open(str(input_file), \"r\") as f:\n",
    "        lbls = json.load(f)\n",
    "\n",
    "    for lbl in lbls:\n",
    "        # we check if the label already exists, in case this cell was already executed\n",
    "        context_stable_ids = \"~~\".join((lbl['performance'], lbl['director']))\n",
    "        query = session.query(StableLabel).filter(StableLabel.context_stable_ids == context_stable_ids)\n",
    "        query = query.filter(StableLabel.annotator_name == annotator_name)\n",
    "        if query.count() == 0:\n",
    "            session.add(StableLabel(\n",
    "                context_stable_ids=context_stable_ids,\n",
    "                annotator_name=annotator_name,\n",
    "                value=lbl['value']\n",
    "            ))\n",
    "\n",
    "    # commit session\n",
    "    session.commit()\n",
    "\n",
    "    # reload annotator labels\n",
    "    reload_annotator_labels(session, performance_with_director, annotator_name, split=split, filter_label_split=False)\n",
    "\n",
    "reload_external_labels(session, \"saved_gold.json\", \"gold\", split=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Labeling Functions (LFs)\n",
    "\n",
    "Define the LFs which Snorkel uses to create noise-aware training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.lf_helpers import (\n",
    "    get_left_tokens, get_right_tokens, get_between_tokens,\n",
    "    get_text_between, get_tagged_text,\n",
    ")\n",
    "\n",
    "\n",
    "FALSE = -1\n",
    "ABSTAIN = 0\n",
    "TRUE = 1\n",
    "\n",
    "re1 = re.compile(r\"[A-Z][a-z]*\\s[A-Z][a-z]*\")    \n",
    "def LF_2_capitalized_seq_P(c):\n",
    "    match = re1.findall(c.performance.get_span())\n",
    "    if len(match) > 0: \n",
    "        return TRUE\n",
    "    else:\n",
    "        return FALSE    \n",
    "def LF_2_capitalized_seq_D(c):\n",
    "    match = re1.findall(c.director.get_span())\n",
    "    if len(match) > 0: \n",
    "        return TRUE\n",
    "    else:\n",
    "        return FALSE\n",
    "    \n",
    "re2 = re.compile(r\"[A-Z][a-z]*\\s[a-z]*\\s[A-Z][a-z]*\")\n",
    "def LF_upper_lower_upper_P(c):\n",
    "    match = re2.findall(c.performance.get_span())\n",
    "    if len(match) > 0: \n",
    "        return TRUE\n",
    "    else:\n",
    "        return ABSTAIN      \n",
    "\n",
    "re4 = re.compile(r'\"[^\\']+\"')\n",
    "def LF_double_quotes_P(c):\n",
    "    match = re4.findall(c.performance.get_span())\n",
    "    if len(match) > 0: \n",
    "        return TRUE\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "def LF_double_quotes_D(c):\n",
    "    match = re4.findall(c.director.get_span())\n",
    "    if len(match) > 0: \n",
    "        return FALSE\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "    \n",
    "re6 = re.compile(r\"([a-zA-Z']+)(?=\\S*['])\")\n",
    "def LF_apostrophe_D(c):\n",
    "    match = re6.findall(c.director.get_span())\n",
    "    if len(match) > 0: \n",
    "        return TRUE\n",
    "    else:\n",
    "        return ABSTAIN  \n",
    "    \n",
    "re7 = re.compile(r\"'[^']+'\")\n",
    "def LF_single_quotes_P(c):\n",
    "    match = re7.findall(c.performance.get_span())\n",
    "    if len(match) > 0: \n",
    "        return TRUE\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "def LF_single_quotes_D(c):\n",
    "    match = re7.findall(c.director.get_span())\n",
    "    if len(match) > 0: \n",
    "        return FALSE\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "    \n",
    "re3 = re.compile(r\"[A-Z][a-z]*\\s\\(\\d{4}\\)\")\n",
    "def LF_year_P(c):\n",
    "    match = re3.findall(c.performance.get_span())\n",
    "    if len(match) > 0: \n",
    "        return TRUE\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "    \n",
    "def LF_quote_or_year(c):\n",
    "    m1 = re7.findall(c.performance.get_span())\n",
    "    m2 = re3.findall(c.performance.get_span())\n",
    "    match = m1 + m2\n",
    "    if len(match) > 0: \n",
    "        return TRUE\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "\n",
    "re10 = re.compile(r\"(?<=\\')[\\w\\s]+(?=\\.|\\,)\")\n",
    "def LF_possesive_P(c):\n",
    "    match = re10.findall(c.performance.get_span())\n",
    "    if len(match) > 0: \n",
    "        return TRUE\n",
    "    else:\n",
    "        return ABSTAIN \n",
    "def LF_possesive_D(c):\n",
    "    match = re10.findall(c.director.get_span())\n",
    "    if len(match) > 0: \n",
    "        return FALSE\n",
    "    else:\n",
    "        return ABSTAIN \n",
    "\n",
    "    \n",
    "re11 = re.compile(r\"[A-Z][a-z]*\\s[A-Z][a-z]*\\s[A-Z][a-z]*\")    \n",
    "def LF_3_capitalized_seq_P(c):\n",
    "    match = re11.findall(c.performance.get_span())\n",
    "    if len(match) > 0: \n",
    "        return TRUE\n",
    "    else:\n",
    "        return ABSTAIN    \n",
    "def LF_3_capitalized_seq_D(c):\n",
    "    match = re11.findall(c.director.get_span())\n",
    "    if len(match) > 0: \n",
    "        return TRUE\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "    \n",
    "    \n",
    "re12 = re.compile(r\"[A-Z][a-z]*\\s[A-Z][a-z]*'\")  \n",
    "def LF_2_titlecase_apostrophe_D(c):\n",
    "    match = re12.findall(c.director.get_span())\n",
    "    if len(match) > 0: \n",
    "        return TRUE\n",
    "    else:\n",
    "        return ABSTAIN   \n",
    "    \n",
    "    \n",
    "re13 = re.compile(r\"[A-Z][a-z]*'\")  \n",
    "def LF_1_titlecase_apostrophe_D(c):\n",
    "    match = re13.findall(c.director.get_span())\n",
    "    if len(match) > 0: \n",
    "        return TRUE\n",
    "    else:\n",
    "        return ABSTAIN   \n",
    "def LF_1_titlecase_apostrophe_P(c):\n",
    "    match = re13.findall(c.performance.get_span())\n",
    "    if len(match) > 0: \n",
    "        return ABSTAIN\n",
    "    else:\n",
    "        return ABSTAIN\n",
    "    \n",
    "    \n",
    "re15 = re.compile(r\"[A-Z][a-z][a-z]+\")    \n",
    "def LF_titlecase_P(c):\n",
    "    match = re15.findall(c.performance.get_span())\n",
    "    if len(match) > 0: \n",
    "        return ABSTAIN\n",
    "    else:\n",
    "        return FALSE \n",
    "    \n",
    "      \n",
    "re16 = re.compile(r\"[A-Z][a-z][a-z]+\\s[A-Z][a-z][a-z]+\")    \n",
    "def LF_titlecase_D(c):\n",
    "    match = re16.findall(c.director.get_span())\n",
    "    if len(match) > 0: \n",
    "        return ABSTAIN\n",
    "    else:\n",
    "        return FALSE\n",
    "\n",
    "\n",
    "re14 = re.compile(r\"\\d\")    \n",
    "def LF_digit_P(c):\n",
    "    match = re14.findall(c.performance.get_span())\n",
    "    if len(match) > 0: \n",
    "        return TRUE\n",
    "    else:\n",
    "        return ABSTAIN    \n",
    "\n",
    "\n",
    "re17 = re.compile(\"directed by\")  \n",
    "re15 = re.compile(r\"[A-Z][a-z][a-z]+\")  \n",
    "def LF_movie_direct_P(c):\n",
    "    m1 = re17.findall(c.performance.get_span())\n",
    "    m2 = re15.findall(c.performance.get_span())\n",
    "    if len(m1) > 0 and len(m2) == 0: \n",
    "        return FALSE\n",
    "    else:\n",
    "        return ABSTAIN \n",
    "    \n",
    "\n",
    "re18 = re.compile(\"comedy\") \n",
    "re19 = re.compile(\"drama\")\n",
    "re20 = re.compile(\"horror\")\n",
    "re21 = re.compile(\"reuniting\")\n",
    "re22 = re.compile(\"romance\")\n",
    "re23 = re.compile(\"film\")\n",
    "re24 = re.compile(\"award\")\n",
    "def LF_genre_D(c):\n",
    "    m1 = re18.findall(c.director.get_span())\n",
    "    m2 = re19.findall(c.director.get_span())\n",
    "    m3 = re20.findall(c.director.get_span())\n",
    "    m4 = re21.findall(c.director.get_span())\n",
    "    m5 = re22.findall(c.director.get_span())\n",
    "    m6 = re23.findall(c.director.get_span())\n",
    "    m7 = re24.findall(c.director.get_span())\n",
    "    match = m1 + m2 + m3 + m4 + m5 + m6 + m7\n",
    "    if len(match) > 0: \n",
    "        return FALSE\n",
    "    else:\n",
    "        return ABSTAIN   \n",
    "def LF_genre_P(c):\n",
    "    m1 = re18.findall(c.performance.get_span())\n",
    "    m2 = re19.findall(c.performance.get_span())\n",
    "    m3 = re20.findall(c.performance.get_span())\n",
    "    m4 = re21.findall(c.performance.get_span())\n",
    "    m5 = re22.findall(c.performance.get_span())\n",
    "    m6 = re23.findall(c.performance.get_span())\n",
    "    m7 = re24.findall(c.performance.get_span())\n",
    "    match = m1 + m2 + m3 + m4 + m5 + m6 + m7\n",
    "    if len(match) > 0: \n",
    "        return FALSE\n",
    "    else:\n",
    "        return ABSTAIN \n",
    "    \n",
    "re26 = re.compile('act')\n",
    "def LF_actors_P(c):\n",
    "    match = re26.findall(c.performance.get_span())\n",
    "    if len(match) > 0: \n",
    "        return FALSE\n",
    "    else:\n",
    "        return ABSTAIN \n",
    "    \n",
    "    \n",
    "    \n",
    "def LF_first_capitalized_D(c):\n",
    "    string = c.director.get_span()\n",
    "    if string[0].islower():\n",
    "        return FALSE\n",
    "    else:\n",
    "        return ABSTAIN   \n",
    "def LF_first_capitalized_P(c):\n",
    "    string = c.performance.get_span()\n",
    "    if string[0].islower():\n",
    "        return FALSE\n",
    "    else:\n",
    "        return ABSTAIN  \n",
    "    \n",
    "def LF_nearby_5(c):\n",
    "    if len(list(get_between_tokens(c))) < 5:\n",
    "        return TRUE \n",
    "    else:\n",
    "        return ABSTAIN\n",
    "    \n",
    "def LF_directed_by(c):\n",
    "    bet_text = get_text_between(c)\n",
    "    if \"directed\" in bet_text:\n",
    "        return TRUE\n",
    "    else: \n",
    "        return ABSTAIN\n",
    "    \n",
    "def LF_parenthesis_right(c):\n",
    "    right_tokens = get_right_tokens(c)\n",
    "    if \"(\" in ' '.join(right_tokens):\n",
    "        return TRUE \n",
    "    else: \n",
    "        return ABSTAIN\n",
    "\n",
    "regex_yb = re.compile(r\"\\d{4}\")\n",
    "def LF_year_between(c):\n",
    "    bet_tokens = ' '.join(get_between_tokens(c))\n",
    "    match = regex_yb.findall(bet_tokens)\n",
    "    if match:\n",
    "        return FALSE \n",
    "    else: \n",
    "        return ABSTAIN\n",
    "\n",
    "def LF_star(c):\n",
    "    bet_tokens = get_between_tokens(c)\n",
    "    if \"star\" in ' '.join(bet_tokens):\n",
    "        return FALSE \n",
    "    else: \n",
    "        return ABSTAIN\n",
    "    \n",
    "def LF_act(c):\n",
    "    bet_tokens = get_between_tokens(c)\n",
    "    if \"act\" in ' '.join(bet_tokens):\n",
    "        return FALSE \n",
    "    else: \n",
    "        return ABSTAIN\n",
    "    \n",
    "def LF_with(c):\n",
    "    bet_tokens = get_between_tokens(c)\n",
    "    if \"with\" in ' '.join(bet_tokens):\n",
    "        return FALSE \n",
    "    else: \n",
    "        return ABSTAIN\n",
    "    \n",
    "def LF_feat(c):\n",
    "    bet_tokens = get_between_tokens(c)\n",
    "    if \"feat\" in ' '.join(bet_tokens):\n",
    "        return FALSE \n",
    "    else: \n",
    "        return ABSTAIN\n",
    "    \n",
    "def LF_featuring(c):\n",
    "    bet_tokens = get_between_tokens(c)\n",
    "    if \"featuring\" in ' '.join(bet_tokens):\n",
    "        return FALSE \n",
    "    else: \n",
    "        return ABSTAIN\n",
    "    \n",
    "def LF_opposite(c):\n",
    "    bet_tokens = get_between_tokens(c)\n",
    "    if \"opposite\" in ' '.join(bet_tokens):\n",
    "        return FALSE \n",
    "    else: \n",
    "        return ABSTAIN\n",
    "\n",
    "    \n",
    "def LF_which(c):\n",
    "    bet_tokens = get_between_tokens(c)\n",
    "    if \"which\" in ' '.join(bet_tokens):\n",
    "        return FALSE \n",
    "    else: \n",
    "        return ABSTAIN\n",
    "    \n",
    "def LF_film(c):\n",
    "    bet_tokens = get_between_tokens(c)\n",
    "    if \"film\" in ' '.join(bet_tokens):\n",
    "        return FALSE \n",
    "    else: \n",
    "        return ABSTAIN\n",
    "    \n",
    "def LF_work(c):\n",
    "    bet_tokens = get_between_tokens(c)\n",
    "    if \"work\" in ' '.join(bet_tokens):\n",
    "        return FALSE \n",
    "    else: \n",
    "        return ABSTAIN\n",
    "    \n",
    "def LF_based(c):\n",
    "    bet_text = get_text_between(c)\n",
    "    if \"based\" in bet_text or \"earlier\" in bet_text:\n",
    "        return FALSE \n",
    "    else: \n",
    "        return ABSTAIN\n",
    "    \n",
    "def LF_drama(c):\n",
    "    bet_text = get_text_between(c)\n",
    "    if \"drama\" in bet_text: \n",
    "        return FALSE \n",
    "    else: \n",
    "        return ABSTAIN\n",
    "    \n",
    "def LF_nominat(c):\n",
    "    bet_text = get_text_between(c)\n",
    "    if \"nominat\" in bet_text or \"feat\" in bet_text: \n",
    "        return FALSE \n",
    "    else: \n",
    "        return ABSTAIN\n",
    "    \n",
    "def LF_final(c):\n",
    "    bet_text = get_text_between(c)\n",
    "    if \"final\" in bet_text:\n",
    "        return FALSE \n",
    "    else: \n",
    "        return ABSTAIN\n",
    "    \n",
    "def LF_reunit(c):\n",
    "    bet_text = get_text_between(c)\n",
    "    if \"reunit\" in bet_text:\n",
    "        return FALSE \n",
    "    else: \n",
    "        return ABSTAIN\n",
    "    \n",
    "def LF_document(c):\n",
    "    bet_text = get_text_between(c)\n",
    "    if \"document\" in bet_text or \"film\" in bet_text:\n",
    "        return FALSE \n",
    "    else: \n",
    "        return ABSTAIN\n",
    "\n",
    "re15 = re.compile(r\"[A-Z][a-z][a-z]+\") \n",
    "def LF_names_between(c):\n",
    "    bet_text = get_text_between(c)\n",
    "    match = re15.findall(bet_text)\n",
    "    if len(match) >= 3:\n",
    "        return FALSE \n",
    "    else: \n",
    "        return ABSTAIN\n",
    "    \n",
    "re15 = re.compile(r\"[A-Z][a-z][a-z]+\") \n",
    "def LF_partners(c):\n",
    "    bet_text = get_text_between(c)\n",
    "    match = re15.findall(bet_text)\n",
    "    if len(match) >= 3 and 'opposite' in bet_text:\n",
    "        return FALSE \n",
    "    else: \n",
    "        return ABSTAIN\n",
    "\n",
    "re3 = re.compile(r\"[A-Z][a-z]*\\s\\(\\d{4}\\)\")\n",
    "def LF_movie_between(c):\n",
    "    bet_text = get_text_between(c)\n",
    "    match = re3.findall(bet_text)\n",
    "    if len(match) > 0:\n",
    "        return FALSE \n",
    "    else: \n",
    "        return ABSTAIN   \n",
    "    \n",
    "re25 = re.compile(\",\")\n",
    "def LF_commas_between(c):\n",
    "    bet_text = get_text_between(c)\n",
    "    match = re25.findall(bet_text)\n",
    "    if len(match) >= 2:\n",
    "        return FALSE \n",
    "    else: \n",
    "        return ABSTAIN  \n",
    "    \n",
    "def LF_relations(c):\n",
    "    bet_text = get_text_between(c)\n",
    "    if 'and the' in bet_text or 'and The' in bet_text or 'was a' in bet_text:\n",
    "        return FALSE \n",
    "    else: \n",
    "        return ABSTAIN  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all labeling functions\n",
    "performance_with_director_lfs = [\n",
    "    LF_upper_lower_upper_P,\n",
    "    LF_year_P,  LF_single_quotes_P, LF_single_quotes_D,\n",
    "    LF_1_titlecase_apostrophe_D, LF_1_titlecase_apostrophe_P, LF_digit_P,\n",
    "    LF_directed_by, LF_parenthesis_right, LF_year_between,\n",
    "    LF_star, LF_act, LF_with, LF_commas_between,\n",
    "    LF_feat, LF_featuring, LF_opposite, \n",
    "    LF_titlecase_P, LF_titlecase_D,\n",
    "    LF_which, LF_film, LF_work, LF_based, LF_quote_or_year,\n",
    "    LF_movie_direct_P, LF_genre_D, LF_genre_P, \n",
    "    LF_drama, LF_nominat, LF_final, LF_reunit, LF_document,\n",
    "    LF_first_capitalized_D, LF_first_capitalized_P, LF_names_between, \n",
    "    LF_partners, LF_movie_between,\n",
    "    LF_actors_P, LF_relations,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Generative Model\n",
    "\n",
    "Now, we'll train a model of the LFs to estimate their accuracies. Once the model is trained, we can combine the outputs of the LFs into a single, noise-aware training label set for our extractor. Intuitively, we'll model the LFs by observing how they overlap and conflict with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1701)\n",
    "\n",
    "labeler = LabelAnnotator(lfs=performance_with_director_lfs)\n",
    "L_train = labeler.apply(split=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get detailed statistics of LFs before training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Performance Before Generative Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LF_upper_lower_upper_P</th>\n",
       "      <td>0</td>\n",
       "      <td>0.103448</td>\n",
       "      <td>0.103448</td>\n",
       "      <td>0.103448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_year_P</th>\n",
       "      <td>1</td>\n",
       "      <td>0.379310</td>\n",
       "      <td>0.379310</td>\n",
       "      <td>0.367816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_single_quotes_P</th>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_single_quotes_D</th>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_1_titlecase_apostrophe_D</th>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_1_titlecase_apostrophe_P</th>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_digit_P</th>\n",
       "      <td>6</td>\n",
       "      <td>0.390805</td>\n",
       "      <td>0.390805</td>\n",
       "      <td>0.379310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_directed_by</th>\n",
       "      <td>7</td>\n",
       "      <td>0.011494</td>\n",
       "      <td>0.011494</td>\n",
       "      <td>0.011494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_parenthesis_right</th>\n",
       "      <td>8</td>\n",
       "      <td>0.425287</td>\n",
       "      <td>0.425287</td>\n",
       "      <td>0.413793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_year_between</th>\n",
       "      <td>9</td>\n",
       "      <td>0.287356</td>\n",
       "      <td>0.287356</td>\n",
       "      <td>0.287356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_star</th>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_act</th>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_with</th>\n",
       "      <td>12</td>\n",
       "      <td>0.011494</td>\n",
       "      <td>0.011494</td>\n",
       "      <td>0.011494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_commas_between</th>\n",
       "      <td>13</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>0.103448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_feat</th>\n",
       "      <td>14</td>\n",
       "      <td>0.011494</td>\n",
       "      <td>0.011494</td>\n",
       "      <td>0.011494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_featuring</th>\n",
       "      <td>15</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_opposite</th>\n",
       "      <td>16</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_titlecase_P</th>\n",
       "      <td>17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_titlecase_D</th>\n",
       "      <td>18</td>\n",
       "      <td>0.597701</td>\n",
       "      <td>0.574713</td>\n",
       "      <td>0.459770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_which</th>\n",
       "      <td>19</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_film</th>\n",
       "      <td>20</td>\n",
       "      <td>0.149425</td>\n",
       "      <td>0.149425</td>\n",
       "      <td>0.080460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_work</th>\n",
       "      <td>21</td>\n",
       "      <td>0.011494</td>\n",
       "      <td>0.011494</td>\n",
       "      <td>0.011494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_based</th>\n",
       "      <td>22</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_quote_or_year</th>\n",
       "      <td>23</td>\n",
       "      <td>0.379310</td>\n",
       "      <td>0.379310</td>\n",
       "      <td>0.367816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_movie_direct_P</th>\n",
       "      <td>24</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_genre_D</th>\n",
       "      <td>25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_genre_P</th>\n",
       "      <td>26</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>0.034483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_drama</th>\n",
       "      <td>27</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_nominat</th>\n",
       "      <td>28</td>\n",
       "      <td>0.011494</td>\n",
       "      <td>0.011494</td>\n",
       "      <td>0.011494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_final</th>\n",
       "      <td>29</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_reunit</th>\n",
       "      <td>30</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_document</th>\n",
       "      <td>31</td>\n",
       "      <td>0.149425</td>\n",
       "      <td>0.149425</td>\n",
       "      <td>0.080460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_first_capitalized_D</th>\n",
       "      <td>32</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_first_capitalized_P</th>\n",
       "      <td>33</td>\n",
       "      <td>0.310345</td>\n",
       "      <td>0.298851</td>\n",
       "      <td>0.183908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_names_between</th>\n",
       "      <td>34</td>\n",
       "      <td>0.367816</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>0.103448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_partners</th>\n",
       "      <td>35</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_movie_between</th>\n",
       "      <td>36</td>\n",
       "      <td>0.045977</td>\n",
       "      <td>0.045977</td>\n",
       "      <td>0.045977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_actors_P</th>\n",
       "      <td>37</td>\n",
       "      <td>0.022989</td>\n",
       "      <td>0.022989</td>\n",
       "      <td>0.022989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_relations</th>\n",
       "      <td>38</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              j  Coverage  Overlaps  Conflicts\n",
       "LF_upper_lower_upper_P        0  0.103448  0.103448   0.103448\n",
       "LF_year_P                     1  0.379310  0.379310   0.367816\n",
       "LF_single_quotes_P            2  0.000000  0.000000   0.000000\n",
       "LF_single_quotes_D            3  0.000000  0.000000   0.000000\n",
       "LF_1_titlecase_apostrophe_D   4  0.000000  0.000000   0.000000\n",
       "LF_1_titlecase_apostrophe_P   5  0.000000  0.000000   0.000000\n",
       "LF_digit_P                    6  0.390805  0.390805   0.379310\n",
       "LF_directed_by                7  0.011494  0.011494   0.011494\n",
       "LF_parenthesis_right          8  0.425287  0.425287   0.413793\n",
       "LF_year_between               9  0.287356  0.287356   0.287356\n",
       "LF_star                      10  0.000000  0.000000   0.000000\n",
       "LF_act                       11  0.000000  0.000000   0.000000\n",
       "LF_with                      12  0.011494  0.011494   0.011494\n",
       "LF_commas_between            13  0.344828  0.344828   0.103448\n",
       "LF_feat                      14  0.011494  0.011494   0.011494\n",
       "LF_featuring                 15  0.000000  0.000000   0.000000\n",
       "LF_opposite                  16  0.000000  0.000000   0.000000\n",
       "LF_titlecase_P               17  0.000000  0.000000   0.000000\n",
       "LF_titlecase_D               18  0.597701  0.574713   0.459770\n",
       "LF_which                     19  0.000000  0.000000   0.000000\n",
       "LF_film                      20  0.149425  0.149425   0.080460\n",
       "LF_work                      21  0.011494  0.011494   0.011494\n",
       "LF_based                     22  0.000000  0.000000   0.000000\n",
       "LF_quote_or_year             23  0.379310  0.379310   0.367816\n",
       "LF_movie_direct_P            24  0.000000  0.000000   0.000000\n",
       "LF_genre_D                   25  0.000000  0.000000   0.000000\n",
       "LF_genre_P                   26  0.068966  0.068966   0.034483\n",
       "LF_drama                     27  0.000000  0.000000   0.000000\n",
       "LF_nominat                   28  0.011494  0.011494   0.011494\n",
       "LF_final                     29  0.000000  0.000000   0.000000\n",
       "LF_reunit                    30  0.000000  0.000000   0.000000\n",
       "LF_document                  31  0.149425  0.149425   0.080460\n",
       "LF_first_capitalized_D       32  0.000000  0.000000   0.000000\n",
       "LF_first_capitalized_P       33  0.310345  0.298851   0.183908\n",
       "LF_names_between             34  0.367816  0.344828   0.103448\n",
       "LF_partners                  35  0.000000  0.000000   0.000000\n",
       "LF_movie_between             36  0.045977  0.045977   0.045977\n",
       "LF_actors_P                  37  0.022989  0.022989   0.022989\n",
       "LF_relations                 38  0.000000  0.000000   0.000000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_train.lf_stats(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Report the weights of the LFs after generative model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferred cardinality: 2\n",
      "LF weights: [ 0.19427958  0.6262435   0.06462321  0.0473138   0.07642905  0.092425\n",
      "  0.62782561  0.07128648  0.4226728  -0.04638931  0.08068634  0.06897329\n",
      "  0.06805461  0.52322153  0.08839914  0.07982269  0.07711804  0.07006701\n",
      " -0.20495203  0.09041777  0.21279293  0.07880266  0.08296787  0.6146284\n",
      "  0.06758788  0.06471318  0.1371921   0.0850499   0.09235736  0.0805227\n",
      "  0.05463209  0.20287494  0.10353593  0.02409021  0.54250966  0.07087675\n",
      " -0.03227999  0.04385748  0.09728198]\n"
     ]
    }
   ],
   "source": [
    "from snorkel.learning import GenerativeModel\n",
    "\n",
    "gen_model = GenerativeModel()\n",
    "gen_model.train(L_train, epochs=100, decay=0.95, step_size=0.1 / L_train.shape[0], reg_param=1e-6)\n",
    "\n",
    "print(\"LF weights:\", gen_model.weights.lf_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have learned the generative model, we will measure its performances using the provided test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_gold_dev = load_gold_labels(session, annotator_name='gold', split=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.558\n",
      "Neg. class accuracy: 0.677\n",
      "Precision            0.796\n",
      "Recall               0.558\n",
      "F1                   0.656\n",
      "----------------------------------------\n",
      "TP: 82 | FP: 21 | TN: 44 | FN: 65\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "L_dev = labeler.apply_existing(split=1)\n",
    "tp, fp, tn, fn = gen_model.error_analysis(session, L_dev, L_gold_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get detailed statistics of LFs learned by the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Report the performance of your LFs after generative model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Matheus\\Anaconda3\\lib\\site-packages\\snorkel\\annotations.py:137: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ac = (tp+tn) / (tp+tn+fp+fn)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>TN</th>\n",
       "      <th>Empirical Acc.</th>\n",
       "      <th>Learned Acc.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LF_upper_lower_upper_P</th>\n",
       "      <td>0</td>\n",
       "      <td>0.047170</td>\n",
       "      <td>0.023585</td>\n",
       "      <td>0.023585</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.587151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_year_P</th>\n",
       "      <td>1</td>\n",
       "      <td>0.216981</td>\n",
       "      <td>0.216981</td>\n",
       "      <td>0.174528</td>\n",
       "      <td>30</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.776944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_single_quotes_P</th>\n",
       "      <td>2</td>\n",
       "      <td>0.084906</td>\n",
       "      <td>0.084906</td>\n",
       "      <td>0.066038</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.530216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_single_quotes_D</th>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.536373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_1_titlecase_apostrophe_D</th>\n",
       "      <td>4</td>\n",
       "      <td>0.099057</td>\n",
       "      <td>0.094340</td>\n",
       "      <td>0.089623</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.527148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_1_titlecase_apostrophe_P</th>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.541905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_digit_P</th>\n",
       "      <td>6</td>\n",
       "      <td>0.297170</td>\n",
       "      <td>0.297170</td>\n",
       "      <td>0.245283</td>\n",
       "      <td>47</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.746032</td>\n",
       "      <td>0.772901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_directed_by</th>\n",
       "      <td>7</td>\n",
       "      <td>0.325472</td>\n",
       "      <td>0.311321</td>\n",
       "      <td>0.297170</td>\n",
       "      <td>55</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.797101</td>\n",
       "      <td>0.537723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_parenthesis_right</th>\n",
       "      <td>8</td>\n",
       "      <td>0.136792</td>\n",
       "      <td>0.117925</td>\n",
       "      <td>0.108491</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.827586</td>\n",
       "      <td>0.700797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_year_between</th>\n",
       "      <td>9</td>\n",
       "      <td>0.193396</td>\n",
       "      <td>0.193396</td>\n",
       "      <td>0.174528</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>11</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.475921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_star</th>\n",
       "      <td>10</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.535799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_act</th>\n",
       "      <td>11</td>\n",
       "      <td>0.047170</td>\n",
       "      <td>0.037736</td>\n",
       "      <td>0.014151</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.528386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_with</th>\n",
       "      <td>12</td>\n",
       "      <td>0.056604</td>\n",
       "      <td>0.056604</td>\n",
       "      <td>0.028302</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.540357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_commas_between</th>\n",
       "      <td>13</td>\n",
       "      <td>0.207547</td>\n",
       "      <td>0.207547</td>\n",
       "      <td>0.202830</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>15</td>\n",
       "      <td>0.340909</td>\n",
       "      <td>0.736872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_feat</th>\n",
       "      <td>14</td>\n",
       "      <td>0.056604</td>\n",
       "      <td>0.056604</td>\n",
       "      <td>0.037736</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.545590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_featuring</th>\n",
       "      <td>15</td>\n",
       "      <td>0.047170</td>\n",
       "      <td>0.047170</td>\n",
       "      <td>0.028302</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.542001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_opposite</th>\n",
       "      <td>16</td>\n",
       "      <td>0.037736</td>\n",
       "      <td>0.037736</td>\n",
       "      <td>0.028302</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.547173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_titlecase_P</th>\n",
       "      <td>17</td>\n",
       "      <td>0.146226</td>\n",
       "      <td>0.146226</td>\n",
       "      <td>0.080189</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>10</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>0.538843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_titlecase_D</th>\n",
       "      <td>18</td>\n",
       "      <td>0.212264</td>\n",
       "      <td>0.212264</td>\n",
       "      <td>0.169811</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>13</td>\n",
       "      <td>0.288889</td>\n",
       "      <td>0.401810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_which</th>\n",
       "      <td>19</td>\n",
       "      <td>0.014151</td>\n",
       "      <td>0.014151</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.546456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_film</th>\n",
       "      <td>20</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.601021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_work</th>\n",
       "      <td>21</td>\n",
       "      <td>0.018868</td>\n",
       "      <td>0.018868</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.539415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_based</th>\n",
       "      <td>22</td>\n",
       "      <td>0.023585</td>\n",
       "      <td>0.023585</td>\n",
       "      <td>0.023585</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.536509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_quote_or_year</th>\n",
       "      <td>23</td>\n",
       "      <td>0.301887</td>\n",
       "      <td>0.301887</td>\n",
       "      <td>0.240566</td>\n",
       "      <td>45</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.703125</td>\n",
       "      <td>0.776810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_movie_direct_P</th>\n",
       "      <td>24</td>\n",
       "      <td>0.099057</td>\n",
       "      <td>0.099057</td>\n",
       "      <td>0.042453</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.532185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_genre_D</th>\n",
       "      <td>25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.529315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_genre_P</th>\n",
       "      <td>26</td>\n",
       "      <td>0.221698</td>\n",
       "      <td>0.221698</td>\n",
       "      <td>0.146226</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>30</td>\n",
       "      <td>0.638298</td>\n",
       "      <td>0.572276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_drama</th>\n",
       "      <td>27</td>\n",
       "      <td>0.028302</td>\n",
       "      <td>0.028302</td>\n",
       "      <td>0.028302</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.548436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_nominat</th>\n",
       "      <td>28</td>\n",
       "      <td>0.056604</td>\n",
       "      <td>0.056604</td>\n",
       "      <td>0.037736</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.551206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_final</th>\n",
       "      <td>29</td>\n",
       "      <td>0.014151</td>\n",
       "      <td>0.014151</td>\n",
       "      <td>0.014151</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.543385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_reunit</th>\n",
       "      <td>30</td>\n",
       "      <td>0.018868</td>\n",
       "      <td>0.018868</td>\n",
       "      <td>0.018868</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.523282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_document</th>\n",
       "      <td>31</td>\n",
       "      <td>0.018868</td>\n",
       "      <td>0.018868</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.606484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_first_capitalized_D</th>\n",
       "      <td>32</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.547261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_first_capitalized_P</th>\n",
       "      <td>33</td>\n",
       "      <td>0.466981</td>\n",
       "      <td>0.452830</td>\n",
       "      <td>0.367925</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "      <td>34</td>\n",
       "      <td>0.343434</td>\n",
       "      <td>0.512062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_names_between</th>\n",
       "      <td>34</td>\n",
       "      <td>0.367925</td>\n",
       "      <td>0.363208</td>\n",
       "      <td>0.330189</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>26</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.752292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_partners</th>\n",
       "      <td>35</td>\n",
       "      <td>0.037736</td>\n",
       "      <td>0.037736</td>\n",
       "      <td>0.028302</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.541138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_movie_between</th>\n",
       "      <td>36</td>\n",
       "      <td>0.047170</td>\n",
       "      <td>0.047170</td>\n",
       "      <td>0.047170</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.484490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_actors_P</th>\n",
       "      <td>37</td>\n",
       "      <td>0.014151</td>\n",
       "      <td>0.014151</td>\n",
       "      <td>0.004717</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.518198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_relations</th>\n",
       "      <td>38</td>\n",
       "      <td>0.070755</td>\n",
       "      <td>0.070755</td>\n",
       "      <td>0.070755</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.552162</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              j  Coverage  Overlaps  Conflicts  TP  FP  FN  \\\n",
       "LF_upper_lower_upper_P        0  0.047170  0.023585   0.023585  10   0   0   \n",
       "LF_year_P                     1  0.216981  0.216981   0.174528  30  16   0   \n",
       "LF_single_quotes_P            2  0.084906  0.084906   0.066038  15   3   0   \n",
       "LF_single_quotes_D            3  0.000000  0.000000   0.000000   0   0   0   \n",
       "LF_1_titlecase_apostrophe_D   4  0.099057  0.094340   0.089623  17   4   0   \n",
       "LF_1_titlecase_apostrophe_P   5  0.000000  0.000000   0.000000   0   0   0   \n",
       "LF_digit_P                    6  0.297170  0.297170   0.245283  47  16   0   \n",
       "LF_directed_by                7  0.325472  0.311321   0.297170  55  14   0   \n",
       "LF_parenthesis_right          8  0.136792  0.117925   0.108491  24   5   0   \n",
       "LF_year_between               9  0.193396  0.193396   0.174528   0   0  30   \n",
       "LF_star                      10  0.009434  0.009434   0.009434   0   0   2   \n",
       "LF_act                       11  0.047170  0.037736   0.014151   0   0   1   \n",
       "LF_with                      12  0.056604  0.056604   0.028302   0   0   3   \n",
       "LF_commas_between            13  0.207547  0.207547   0.202830   0   0  29   \n",
       "LF_feat                      14  0.056604  0.056604   0.037736   0   0   1   \n",
       "LF_featuring                 15  0.047170  0.047170   0.028302   0   0   0   \n",
       "LF_opposite                  16  0.037736  0.037736   0.028302   0   0   0   \n",
       "LF_titlecase_P               17  0.146226  0.146226   0.080189   0   0  21   \n",
       "LF_titlecase_D               18  0.212264  0.212264   0.169811   0   0  32   \n",
       "LF_which                     19  0.014151  0.014151   0.009434   0   0   0   \n",
       "LF_film                      20  0.009434  0.009434   0.000000   0   0   0   \n",
       "LF_work                      21  0.018868  0.018868   0.000000   0   0   0   \n",
       "LF_based                     22  0.023585  0.023585   0.023585   0   0   3   \n",
       "LF_quote_or_year             23  0.301887  0.301887   0.240566  45  19   0   \n",
       "LF_movie_direct_P            24  0.099057  0.099057   0.042453   0   0  11   \n",
       "LF_genre_D                   25  0.000000  0.000000   0.000000   0   0   0   \n",
       "LF_genre_P                   26  0.221698  0.221698   0.146226   0   0  17   \n",
       "LF_drama                     27  0.028302  0.028302   0.028302   0   0   2   \n",
       "LF_nominat                   28  0.056604  0.056604   0.037736   0   0   1   \n",
       "LF_final                     29  0.014151  0.014151   0.014151   0   0   0   \n",
       "LF_reunit                    30  0.018868  0.018868   0.018868   0   0   2   \n",
       "LF_document                  31  0.018868  0.018868   0.009434   0   0   1   \n",
       "LF_first_capitalized_D       32  0.009434  0.009434   0.000000   0   0   2   \n",
       "LF_first_capitalized_P       33  0.466981  0.452830   0.367925   0   0  65   \n",
       "LF_names_between             34  0.367925  0.363208   0.330189   0   0  52   \n",
       "LF_partners                  35  0.037736  0.037736   0.028302   0   0   0   \n",
       "LF_movie_between             36  0.047170  0.047170   0.047170   0   0   4   \n",
       "LF_actors_P                  37  0.014151  0.014151   0.004717   0   0   1   \n",
       "LF_relations                 38  0.070755  0.070755   0.070755   0   0   2   \n",
       "\n",
       "                             TN  Empirical Acc.  Learned Acc.  \n",
       "LF_upper_lower_upper_P        0        1.000000      0.587151  \n",
       "LF_year_P                     0        0.652174      0.776944  \n",
       "LF_single_quotes_P            0        0.833333      0.530216  \n",
       "LF_single_quotes_D            0             NaN      0.536373  \n",
       "LF_1_titlecase_apostrophe_D   0        0.809524      0.527148  \n",
       "LF_1_titlecase_apostrophe_P   0             NaN      0.541905  \n",
       "LF_digit_P                    0        0.746032      0.772901  \n",
       "LF_directed_by                0        0.797101      0.537723  \n",
       "LF_parenthesis_right          0        0.827586      0.700797  \n",
       "LF_year_between              11        0.268293      0.475921  \n",
       "LF_star                       0        0.000000      0.535799  \n",
       "LF_act                        9        0.900000      0.528386  \n",
       "LF_with                       9        0.750000      0.540357  \n",
       "LF_commas_between            15        0.340909      0.736872  \n",
       "LF_feat                      11        0.916667      0.545590  \n",
       "LF_featuring                 10        1.000000      0.542001  \n",
       "LF_opposite                   8        1.000000      0.547173  \n",
       "LF_titlecase_P               10        0.322581      0.538843  \n",
       "LF_titlecase_D               13        0.288889      0.401810  \n",
       "LF_which                      3        1.000000      0.546456  \n",
       "LF_film                       2        1.000000      0.601021  \n",
       "LF_work                       4        1.000000      0.539415  \n",
       "LF_based                      2        0.400000      0.536509  \n",
       "LF_quote_or_year              0        0.703125      0.776810  \n",
       "LF_movie_direct_P            10        0.476190      0.532185  \n",
       "LF_genre_D                    0             NaN      0.529315  \n",
       "LF_genre_P                   30        0.638298      0.572276  \n",
       "LF_drama                      4        0.666667      0.548436  \n",
       "LF_nominat                   11        0.916667      0.551206  \n",
       "LF_final                      3        1.000000      0.543385  \n",
       "LF_reunit                     2        0.500000      0.523282  \n",
       "LF_document                   3        0.750000      0.606484  \n",
       "LF_first_capitalized_D        0        0.000000      0.547261  \n",
       "LF_first_capitalized_P       34        0.343434      0.512062  \n",
       "LF_names_between             26        0.333333      0.752292  \n",
       "LF_partners                   8        1.000000      0.541138  \n",
       "LF_movie_between              6        0.600000      0.484490  \n",
       "LF_actors_P                   2        0.666667      0.518198  \n",
       "LF_relations                 13        0.866667      0.552162  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_dev.lf_stats(session, L_gold_dev, gen_model.learned_lf_stats()['Accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now apply the generative model to the training candidates to get the noise-aware training label set. We'll refer to these as the training marginals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_marginals = gen_model.marginals(L_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll look at the distribution of the training marginals:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check distribution of the training marginals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(train_marginals, bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution seems good, given that most classifiers are very close to either 0 or 1, as while it was initially very bad, but as I kept adding more labeling functions based on the FPs and FNs the distribution began to improve. The current distribution differentiates well between the classes, although its clear from the plot that defining what is a match is much easier than defining what is NOT a match."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at some examples in one of the error buckets to improve the LFs. \n",
    "Below are the false positives that we did not correctly label correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "require.undef('viewer');\n",
       "\n",
       "// NOTE: all elements should be selected using this.$el.find to avoid collisions with other Viewers\n",
       "\n",
       "define('viewer', [\"@jupyter-widgets/base\"], function(widgets) {\n",
       "    var ViewerView = widgets.DOMWidgetView.extend({\n",
       "        render: function() {\n",
       "            this.cids   = this.model.get('cids');\n",
       "            this.nPages = this.cids.length;\n",
       "            this.pid  = 0;\n",
       "            this.cxid = 0;\n",
       "            this.cid  = 0;\n",
       "\n",
       "            // Insert the html payload\n",
       "            this.$el.append(this.model.get('html'));\n",
       "\n",
       "            // Initialize all labels from previous sessions\n",
       "            this.labels = this.deserializeDict(this.model.get('_labels_serialized'));\n",
       "            for (var i=0; i < this.nPages; i++) {\n",
       "                this.pid = i;\n",
       "                for (var j=0; j < this.cids[i].length; j++) {\n",
       "                    this.cxid = j;\n",
       "                    for (var k=0; k < this.cids[i][j].length; k++) {\n",
       "                        this.cid = k;\n",
       "                        if (this.cids[i][j][k] in this.labels) {\n",
       "                            this.markCurrentCandidate(false);\n",
       "                        }\n",
       "                    }\n",
       "                }\n",
       "            }\n",
       "            this.pid  = 0;\n",
       "            this.cxid = 0;\n",
       "            this.cid  = 0;\n",
       "\n",
       "            // Enable button functionality for navigation\n",
       "            var that = this;\n",
       "            this.$el.find(\"#next-cand\").click(function() {\n",
       "                that.switchCandidate(1);\n",
       "            });\n",
       "            this.$el.find(\"#prev-cand\").click(function() {\n",
       "                that.switchCandidate(-1);\n",
       "            });\n",
       "            this.$el.find(\"#next-context\").click(function() {\n",
       "                that.switchContext(1);\n",
       "            });\n",
       "            this.$el.find(\"#prev-context\").click(function() {\n",
       "                that.switchContext(-1);\n",
       "            });\n",
       "            this.$el.find(\"#next-page\").click(function() {\n",
       "                that.switchPage(1);\n",
       "            });\n",
       "            this.$el.find(\"#prev-page\").click(function() {\n",
       "                that.switchPage(-1);\n",
       "            });\n",
       "            this.$el.find(\"#label-true\").click(function() {\n",
       "                that.labelCandidate(true, true);\n",
       "            });\n",
       "            this.$el.find(\"#label-false\").click(function() {\n",
       "                that.labelCandidate(false, true);\n",
       "            });\n",
       "\n",
       "            // Arrow key functionality\n",
       "            this.$el.keydown(function(e) {\n",
       "                switch(e.which) {\n",
       "                    case 74: // j\n",
       "                    that.switchCandidate(-1);\n",
       "                    break;\n",
       "\n",
       "                    case 73: // i\n",
       "                    that.switchPage(-1);\n",
       "                    break;\n",
       "\n",
       "                    case 76: // l\n",
       "                    that.switchCandidate(1);\n",
       "                    break;\n",
       "\n",
       "                    case 75: // k\n",
       "                    that.switchPage(1);\n",
       "                    break;\n",
       "\n",
       "                    case 84: // t\n",
       "                    that.labelCandidate(true, true);\n",
       "                    break;\n",
       "\n",
       "                    case 70: // f\n",
       "                    that.labelCandidate(false, true);\n",
       "                    break;\n",
       "                }\n",
       "            });\n",
       "\n",
       "            // Show the first page and highlight the first candidate\n",
       "            this.$el.find(\"#viewer-page-0\").show();\n",
       "            this.switchCandidate(0);\n",
       "        },\n",
       "\n",
       "        // Get candidate selector for currently selected candidate, escaping id properly\n",
       "        getCandidate: function() {\n",
       "            return this.$el.find(\".\"+this.cids[this.pid][this.cxid][this.cid]);\n",
       "        },  \n",
       "\n",
       "        // Color the candidate correctly according to registered label, as well as set highlighting\n",
       "        markCurrentCandidate: function(highlight) {\n",
       "            var cid  = this.cids[this.pid][this.cxid][this.cid];\n",
       "            var tags = this.$el.find(\".\"+cid);\n",
       "\n",
       "            // Clear color classes\n",
       "            tags.removeClass(\"candidate-h\");\n",
       "            tags.removeClass(\"true-candidate\");\n",
       "            tags.removeClass(\"true-candidate-h\");\n",
       "            tags.removeClass(\"false-candidate\");\n",
       "            tags.removeClass(\"false-candidate-h\");\n",
       "            tags.removeClass(\"highlighted\");\n",
       "\n",
       "            if (highlight) {\n",
       "                if (cid in this.labels) {\n",
       "                    tags.addClass(String(this.labels[cid]) + \"-candidate-h\");\n",
       "                } else {\n",
       "                    tags.addClass(\"candidate-h\");\n",
       "                }\n",
       "            \n",
       "            // If un-highlighting, leave with first non-null coloring\n",
       "            } else {\n",
       "                var that = this;\n",
       "                tags.each(function() {\n",
       "                    var cids = $(this).attr('class').split(/\\s+/).map(function(item) {\n",
       "                        return parseInt(item);\n",
       "                    });\n",
       "                    cids.sort();\n",
       "                    for (var i in cids) {\n",
       "                        if (cids[i] in that.labels) {\n",
       "                            var label = that.labels[cids[i]];\n",
       "                            $(this).addClass(String(label) + \"-candidate\");\n",
       "                            $(this).removeClass(String(!label) + \"-candidate\");\n",
       "                            break;\n",
       "                        }\n",
       "                    }\n",
       "                });\n",
       "            }\n",
       "\n",
       "            // Extra highlighting css\n",
       "            if (highlight) {\n",
       "                tags.addClass(\"highlighted\");\n",
       "            }\n",
       "\n",
       "            // Classes for showing direction of relation\n",
       "            if (highlight) {\n",
       "                this.$el.find(\".\"+cid+\"-0\").addClass(\"left-candidate\");\n",
       "                this.$el.find(\".\"+cid+\"-1\").addClass(\"right-candidate\");\n",
       "            } else {\n",
       "                this.$el.find(\".\"+cid+\"-0\").removeClass(\"left-candidate\");\n",
       "                this.$el.find(\".\"+cid+\"-1\").removeClass(\"right-candidate\");\n",
       "            }\n",
       "        },\n",
       "\n",
       "        // Cycle through candidates and highlight, by increment inc\n",
       "        switchCandidate: function(inc) {\n",
       "            var N = this.cids[this.pid].length\n",
       "            var M = this.cids[this.pid][this.cxid].length;\n",
       "            if (N == 0 || M == 0) { return false; }\n",
       "\n",
       "            // Clear highlighting from previous candidate\n",
       "            if (inc != 0) {\n",
       "                this.markCurrentCandidate(false);\n",
       "\n",
       "                // Increment the cid counter\n",
       "\n",
       "                // Move to next context\n",
       "                if (this.cid + inc >= M) {\n",
       "                    while (this.cid + inc >= M) {\n",
       "                        \n",
       "                        // At last context on page, halt\n",
       "                        if (this.cxid == N - 1) {\n",
       "                            this.cid = M - 1;\n",
       "                            inc = 0;\n",
       "                            break;\n",
       "                        \n",
       "                        // Increment to next context\n",
       "                        } else {\n",
       "                            inc -= M - this.cid;\n",
       "                            this.cxid += 1;\n",
       "                            M = this.cids[this.pid][this.cxid].length;\n",
       "                            this.cid = 0;\n",
       "                        }\n",
       "                    }\n",
       "\n",
       "                // Move to previous context\n",
       "                } else if (this.cid + inc < 0) {\n",
       "                    while (this.cid + inc < 0) {\n",
       "                        \n",
       "                        // At first context on page, halt\n",
       "                        if (this.cxid == 0) {\n",
       "                            this.cid = 0;\n",
       "                            inc = 0;\n",
       "                            break;\n",
       "                        \n",
       "                        // Increment to previous context\n",
       "                        } else {\n",
       "                            inc += this.cid + 1;\n",
       "                            this.cxid -= 1;\n",
       "                            M = this.cids[this.pid][this.cxid].length;\n",
       "                            this.cid = M - 1;\n",
       "                        }\n",
       "                    }\n",
       "                }\n",
       "\n",
       "                // Move within current context\n",
       "                this.cid += inc;\n",
       "            }\n",
       "            this.markCurrentCandidate(true);\n",
       "\n",
       "            // Push this new cid to the model\n",
       "            this.model.set('_selected_cid', this.cids[this.pid][this.cxid][this.cid]);\n",
       "            this.touch();\n",
       "        },\n",
       "\n",
       "        // Switch through contexts\n",
       "        switchContext: function(inc) {\n",
       "            this.markCurrentCandidate(false);\n",
       "\n",
       "            // Iterate context on this page\n",
       "            var M = this.cids[this.pid].length;\n",
       "            if (this.cxid + inc < 0) {\n",
       "                this.cxid = 0;\n",
       "            } else if (this.cxid + inc >= M) {\n",
       "                this.cxid = M - 1;\n",
       "            } else {\n",
       "                this.cxid += inc;\n",
       "            }\n",
       "\n",
       "            // Reset cid and set to first candidate\n",
       "            this.cid = 0;\n",
       "            this.switchCandidate(0);\n",
       "        },\n",
       "\n",
       "        // Switch through pages\n",
       "        switchPage: function(inc) {\n",
       "            this.markCurrentCandidate(false);\n",
       "            this.$el.find(\".viewer-page\").hide();\n",
       "            if (this.pid + inc < 0) {\n",
       "                this.pid = 0;\n",
       "            } else if (this.pid + inc > this.nPages - 1) {\n",
       "                this.pid = this.nPages - 1;\n",
       "            } else {\n",
       "                this.pid += inc;\n",
       "            }\n",
       "            this.$el.find(\"#viewer-page-\"+this.pid).show();\n",
       "\n",
       "            // Show pagination\n",
       "            this.$el.find(\"#page\").html(this.pid);\n",
       "\n",
       "            // Reset cid and set to first candidate\n",
       "            this.cid = 0;\n",
       "            this.cxid = 0;\n",
       "            this.switchCandidate(0);\n",
       "        },\n",
       "\n",
       "        // Label currently-selected candidate\n",
       "        labelCandidate: function(label, highlighted) {\n",
       "            var c    = this.getCandidate();\n",
       "            var cid  = this.cids[this.pid][this.cxid][this.cid];\n",
       "            var cl   = String(label) + \"-candidate\";\n",
       "            var clh  = String(label) + \"-candidate-h\";\n",
       "            var cln  = String(!label) + \"-candidate\";\n",
       "            var clnh = String(!label) + \"-candidate-h\";\n",
       "\n",
       "            // Toggle label highlighting\n",
       "            if (c.hasClass(cl) || c.hasClass(clh)) {\n",
       "                c.removeClass(cl);\n",
       "                c.removeClass(clh);\n",
       "                if (highlighted) {\n",
       "                    c.addClass(\"candidate-h\");\n",
       "                }\n",
       "                this.labels[cid] = null;\n",
       "                this.send({event: 'delete_label', cid: cid});\n",
       "            } else {\n",
       "                c.removeClass(cln);\n",
       "                c.removeClass(clnh);\n",
       "                if (highlighted) {\n",
       "                    c.addClass(clh);\n",
       "                } else {\n",
       "                    c.addClass(cl);\n",
       "                }\n",
       "                this.labels[cid] = label;\n",
       "                this.send({event: 'set_label', cid: cid, value: label});\n",
       "            }\n",
       "\n",
       "            // Set the label and pass back to the model\n",
       "            this.model.set('_labels_serialized', this.serializeDict(this.labels));\n",
       "            this.touch();\n",
       "        },\n",
       "\n",
       "        // Serialization of hash maps, because traitlets Dict doesn't seem to work...\n",
       "        serializeDict: function(d) {\n",
       "            var s = [];\n",
       "            for (var key in d) {\n",
       "                s.push(key+\"~~\"+d[key]);\n",
       "            }\n",
       "            return s.join();\n",
       "        },\n",
       "\n",
       "        // Deserialization of hash maps\n",
       "        deserializeDict: function(s) {\n",
       "            var d = {};\n",
       "            var entries = s.split(/,/);\n",
       "            var kv;\n",
       "            for (var i in entries) {\n",
       "                kv = entries[i].split(/~~/);\n",
       "                if (kv[1] == \"true\") {\n",
       "                    d[kv[0]] = true;\n",
       "                } else if (kv[1] == \"false\") {\n",
       "                    d[kv[0]] = false;\n",
       "                }\n",
       "            }\n",
       "            return d;\n",
       "        },\n",
       "    });\n",
       "\n",
       "    return {\n",
       "        ViewerView: ViewerView\n",
       "    };\n",
       "});\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62717008d415446592f675e4e8a6ea3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SentenceNgramViewer(cids=[[[6, 7, 8, 9, 10], [0, 1, 2, 3, 4, 5], [19, 20]], [[11, 12, 13, 14, 15, 16, 17, 18]]â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SentenceNgramViewer(fn, session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Adding Distant Supervision Labeling Function\n",
    "\n",
    "Distant supervision generates training data automatically using an external, imperfectly aligned training resource, such as a Knowledge Base.\n",
    "\n",
    "Defining an additional distant-supervision-based labeling function which DBpedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "titlecase_regex = re.compile('[A-Z][a-z][a-z]+')\n",
    "def LF_distant_supervision(c):\n",
    "        \n",
    "    performance_title = c.performance.get_span()\n",
    "    performance_strings = ' '.join(titlecase_regex.findall(performance_title))\n",
    "    director_name = c.director.get_span()\n",
    "    director_strings = ' '.join(titlecase_regex.findall(director_name))\n",
    "    \n",
    "    sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
    "    sparql.setQuery(f'''\n",
    "        PREFIX dcterms: <http://purl.org/dc/terms/>\n",
    "        PREFIX foaf: <http://xmlns.com/foaf/0.1/>\n",
    "        PREFIX owl: <http://www.w3.org/2002/07/owl#>\n",
    "        PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "        PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "        PREFIX xml: <http://www.w3.org/XML/1998/namespace/>\n",
    "        PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>\n",
    "        PREFIX dbo: <http://dbpedia.org/ontology/>\n",
    "        prefix dbp: <http://dbpedia.org/property/>\n",
    "\n",
    "        SELECT ?movie ?movieTitle ?directorName \n",
    "        WHERE {{\n",
    "        ?movie a dbo:Film .\n",
    "        ?movie foaf:name ?movieTitle .\n",
    "        ?movie dbp:director ?director .\n",
    "        ?director foaf:name ?directorName .\n",
    "\n",
    "        FILTER(CONTAINS(?movieTitle, \"{performance_strings}\"))\n",
    "        FILTER(CONTAINS(?directorName , \"{director_strings}\"))\n",
    "    }}\n",
    "        ''')\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    try:\n",
    "        results = sparql.query().convert()\n",
    "        binding = results['results']['bindings']\n",
    "        movieTitle = binding[0]['movieTitle']['value']\n",
    "        directorName = binding[0]['directorName']['value']\n",
    "        match = True\n",
    "    except:\n",
    "        match = False\n",
    "    if match:\n",
    "        return TRUE\n",
    "    else:\n",
    "        return ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_with_director_lfs = [\n",
    "    LF_upper_lower_upper_P,\n",
    "    LF_year_P,  LF_single_quotes_P, LF_single_quotes_D,\n",
    "    LF_1_titlecase_apostrophe_D, LF_1_titlecase_apostrophe_P, LF_digit_P,\n",
    "    LF_directed_by, LF_parenthesis_right, LF_year_between,\n",
    "    LF_star, LF_act, LF_with, LF_commas_between,\n",
    "    LF_feat, LF_featuring, LF_opposite, \n",
    "    LF_titlecase_P, LF_titlecase_D,\n",
    "    LF_which, LF_film, LF_work, LF_based, LF_quote_or_year,\n",
    "    LF_movie_direct_P, LF_genre_D, LF_genre_P, \n",
    "    LF_drama, LF_nominat, LF_final, LF_reunit, LF_document,\n",
    "    LF_first_capitalized_D, LF_first_capitalized_P, LF_names_between, \n",
    "    LF_partners, LF_movie_between,\n",
    "    LF_actors_P, LF_relations,\n",
    "    LF_distant_supervision\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check LFs Performance Before Generative Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LF_upper_lower_upper_P</th>\n",
       "      <td>0</td>\n",
       "      <td>0.103448</td>\n",
       "      <td>0.103448</td>\n",
       "      <td>0.103448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_year_P</th>\n",
       "      <td>1</td>\n",
       "      <td>0.379310</td>\n",
       "      <td>0.379310</td>\n",
       "      <td>0.367816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_single_quotes_P</th>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_single_quotes_D</th>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_1_titlecase_apostrophe_D</th>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_1_titlecase_apostrophe_P</th>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_digit_P</th>\n",
       "      <td>6</td>\n",
       "      <td>0.390805</td>\n",
       "      <td>0.390805</td>\n",
       "      <td>0.379310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_directed_by</th>\n",
       "      <td>7</td>\n",
       "      <td>0.011494</td>\n",
       "      <td>0.011494</td>\n",
       "      <td>0.011494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_parenthesis_right</th>\n",
       "      <td>8</td>\n",
       "      <td>0.425287</td>\n",
       "      <td>0.425287</td>\n",
       "      <td>0.413793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_year_between</th>\n",
       "      <td>9</td>\n",
       "      <td>0.287356</td>\n",
       "      <td>0.287356</td>\n",
       "      <td>0.287356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_star</th>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_act</th>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_with</th>\n",
       "      <td>12</td>\n",
       "      <td>0.011494</td>\n",
       "      <td>0.011494</td>\n",
       "      <td>0.011494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_commas_between</th>\n",
       "      <td>13</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>0.103448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_feat</th>\n",
       "      <td>14</td>\n",
       "      <td>0.011494</td>\n",
       "      <td>0.011494</td>\n",
       "      <td>0.011494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_featuring</th>\n",
       "      <td>15</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_opposite</th>\n",
       "      <td>16</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_titlecase_P</th>\n",
       "      <td>17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_titlecase_D</th>\n",
       "      <td>18</td>\n",
       "      <td>0.597701</td>\n",
       "      <td>0.574713</td>\n",
       "      <td>0.459770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_which</th>\n",
       "      <td>19</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_film</th>\n",
       "      <td>20</td>\n",
       "      <td>0.149425</td>\n",
       "      <td>0.149425</td>\n",
       "      <td>0.080460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_work</th>\n",
       "      <td>21</td>\n",
       "      <td>0.011494</td>\n",
       "      <td>0.011494</td>\n",
       "      <td>0.011494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_based</th>\n",
       "      <td>22</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_quote_or_year</th>\n",
       "      <td>23</td>\n",
       "      <td>0.379310</td>\n",
       "      <td>0.379310</td>\n",
       "      <td>0.367816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_movie_direct_P</th>\n",
       "      <td>24</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_genre_D</th>\n",
       "      <td>25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_genre_P</th>\n",
       "      <td>26</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>0.034483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_drama</th>\n",
       "      <td>27</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_nominat</th>\n",
       "      <td>28</td>\n",
       "      <td>0.011494</td>\n",
       "      <td>0.011494</td>\n",
       "      <td>0.011494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_final</th>\n",
       "      <td>29</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_reunit</th>\n",
       "      <td>30</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_document</th>\n",
       "      <td>31</td>\n",
       "      <td>0.149425</td>\n",
       "      <td>0.149425</td>\n",
       "      <td>0.080460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_first_capitalized_D</th>\n",
       "      <td>32</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_first_capitalized_P</th>\n",
       "      <td>33</td>\n",
       "      <td>0.310345</td>\n",
       "      <td>0.298851</td>\n",
       "      <td>0.183908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_names_between</th>\n",
       "      <td>34</td>\n",
       "      <td>0.367816</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>0.103448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_partners</th>\n",
       "      <td>35</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_movie_between</th>\n",
       "      <td>36</td>\n",
       "      <td>0.045977</td>\n",
       "      <td>0.045977</td>\n",
       "      <td>0.045977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_actors_P</th>\n",
       "      <td>37</td>\n",
       "      <td>0.022989</td>\n",
       "      <td>0.022989</td>\n",
       "      <td>0.022989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_relations</th>\n",
       "      <td>38</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_distant_supervision</th>\n",
       "      <td>39</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              j  Coverage  Overlaps  Conflicts\n",
       "LF_upper_lower_upper_P        0  0.103448  0.103448   0.103448\n",
       "LF_year_P                     1  0.379310  0.379310   0.367816\n",
       "LF_single_quotes_P            2  0.000000  0.000000   0.000000\n",
       "LF_single_quotes_D            3  0.000000  0.000000   0.000000\n",
       "LF_1_titlecase_apostrophe_D   4  0.000000  0.000000   0.000000\n",
       "LF_1_titlecase_apostrophe_P   5  0.000000  0.000000   0.000000\n",
       "LF_digit_P                    6  0.390805  0.390805   0.379310\n",
       "LF_directed_by                7  0.011494  0.011494   0.011494\n",
       "LF_parenthesis_right          8  0.425287  0.425287   0.413793\n",
       "LF_year_between               9  0.287356  0.287356   0.287356\n",
       "LF_star                      10  0.000000  0.000000   0.000000\n",
       "LF_act                       11  0.000000  0.000000   0.000000\n",
       "LF_with                      12  0.011494  0.011494   0.011494\n",
       "LF_commas_between            13  0.344828  0.344828   0.103448\n",
       "LF_feat                      14  0.011494  0.011494   0.011494\n",
       "LF_featuring                 15  0.000000  0.000000   0.000000\n",
       "LF_opposite                  16  0.000000  0.000000   0.000000\n",
       "LF_titlecase_P               17  0.000000  0.000000   0.000000\n",
       "LF_titlecase_D               18  0.597701  0.574713   0.459770\n",
       "LF_which                     19  0.000000  0.000000   0.000000\n",
       "LF_film                      20  0.149425  0.149425   0.080460\n",
       "LF_work                      21  0.011494  0.011494   0.011494\n",
       "LF_based                     22  0.000000  0.000000   0.000000\n",
       "LF_quote_or_year             23  0.379310  0.379310   0.367816\n",
       "LF_movie_direct_P            24  0.000000  0.000000   0.000000\n",
       "LF_genre_D                   25  0.000000  0.000000   0.000000\n",
       "LF_genre_P                   26  0.068966  0.068966   0.034483\n",
       "LF_drama                     27  0.000000  0.000000   0.000000\n",
       "LF_nominat                   28  0.011494  0.011494   0.011494\n",
       "LF_final                     29  0.000000  0.000000   0.000000\n",
       "LF_reunit                    30  0.000000  0.000000   0.000000\n",
       "LF_document                  31  0.149425  0.149425   0.080460\n",
       "LF_first_capitalized_D       32  0.000000  0.000000   0.000000\n",
       "LF_first_capitalized_P       33  0.310345  0.298851   0.183908\n",
       "LF_names_between             34  0.367816  0.344828   0.103448\n",
       "LF_partners                  35  0.000000  0.000000   0.000000\n",
       "LF_movie_between             36  0.045977  0.045977   0.045977\n",
       "LF_actors_P                  37  0.022989  0.022989   0.022989\n",
       "LF_relations                 38  0.000000  0.000000   0.000000\n",
       "LF_distant_supervision       39  0.000000  0.000000   0.000000"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pre-Train\n",
    "np.random.seed(1701)\n",
    "labeler = LabelAnnotator(lfs=performance_with_director_lfs)\n",
    "L_train = labeler.apply(split=0)\n",
    "L_train.lf_stats(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferred cardinality: 2\n",
      "LF weights: [ 0.19934435  0.61384383  0.06455938  0.04292462  0.08248233  0.08152397\n",
      "  0.58449047  0.0616631   0.42670369 -0.0365053   0.06014309  0.09214489\n",
      "  0.09174504  0.51274644  0.07547928  0.05926433  0.07285599  0.07276436\n",
      " -0.16940715  0.05422622  0.24344786  0.06373718  0.07257566  0.63409584\n",
      "  0.08521251  0.07471332  0.11782195  0.0683143   0.08196633  0.05047449\n",
      "  0.0815411   0.23043219  0.07798068  0.00841972  0.5565986   0.08851671\n",
      "  0.03295081  0.04766071  0.06534586  0.05237717]\n",
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.565\n",
      "Neg. class accuracy: 0.677\n",
      "Precision            0.798\n",
      "Recall               0.565\n",
      "F1                   0.661\n",
      "----------------------------------------\n",
      "TP: 83 | FP: 21 | TN: 44 | FN: 64\n",
      "========================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Matheus\\Anaconda3\\lib\\site-packages\\snorkel\\annotations.py:137: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ac = (tp+tn) / (tp+tn+fp+fn)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>j</th>\n",
       "      <th>Coverage</th>\n",
       "      <th>Overlaps</th>\n",
       "      <th>Conflicts</th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>TN</th>\n",
       "      <th>Empirical Acc.</th>\n",
       "      <th>Learned Acc.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LF_upper_lower_upper_P</th>\n",
       "      <td>0</td>\n",
       "      <td>0.047170</td>\n",
       "      <td>0.023585</td>\n",
       "      <td>0.023585</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.590970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_year_P</th>\n",
       "      <td>1</td>\n",
       "      <td>0.216981</td>\n",
       "      <td>0.216981</td>\n",
       "      <td>0.174528</td>\n",
       "      <td>30</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.772611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_single_quotes_P</th>\n",
       "      <td>2</td>\n",
       "      <td>0.084906</td>\n",
       "      <td>0.084906</td>\n",
       "      <td>0.066038</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.541915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_single_quotes_D</th>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.526420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_1_titlecase_apostrophe_D</th>\n",
       "      <td>4</td>\n",
       "      <td>0.099057</td>\n",
       "      <td>0.094340</td>\n",
       "      <td>0.089623</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.537210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_1_titlecase_apostrophe_P</th>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.546671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_digit_P</th>\n",
       "      <td>6</td>\n",
       "      <td>0.297170</td>\n",
       "      <td>0.297170</td>\n",
       "      <td>0.245283</td>\n",
       "      <td>47</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.746032</td>\n",
       "      <td>0.764621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_directed_by</th>\n",
       "      <td>7</td>\n",
       "      <td>0.325472</td>\n",
       "      <td>0.311321</td>\n",
       "      <td>0.297170</td>\n",
       "      <td>55</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.797101</td>\n",
       "      <td>0.538139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_parenthesis_right</th>\n",
       "      <td>8</td>\n",
       "      <td>0.136792</td>\n",
       "      <td>0.117925</td>\n",
       "      <td>0.108491</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.827586</td>\n",
       "      <td>0.702266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_year_between</th>\n",
       "      <td>9</td>\n",
       "      <td>0.193396</td>\n",
       "      <td>0.193396</td>\n",
       "      <td>0.174528</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>11</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.481129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_star</th>\n",
       "      <td>10</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.522619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_act</th>\n",
       "      <td>11</td>\n",
       "      <td>0.047170</td>\n",
       "      <td>0.037736</td>\n",
       "      <td>0.014151</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.553109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_with</th>\n",
       "      <td>12</td>\n",
       "      <td>0.056604</td>\n",
       "      <td>0.056604</td>\n",
       "      <td>0.028302</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.551020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_commas_between</th>\n",
       "      <td>13</td>\n",
       "      <td>0.207547</td>\n",
       "      <td>0.207547</td>\n",
       "      <td>0.202830</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>15</td>\n",
       "      <td>0.340909</td>\n",
       "      <td>0.743442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_feat</th>\n",
       "      <td>14</td>\n",
       "      <td>0.056604</td>\n",
       "      <td>0.056604</td>\n",
       "      <td>0.037736</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.533938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_featuring</th>\n",
       "      <td>15</td>\n",
       "      <td>0.047170</td>\n",
       "      <td>0.047170</td>\n",
       "      <td>0.028302</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.533858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_opposite</th>\n",
       "      <td>16</td>\n",
       "      <td>0.037736</td>\n",
       "      <td>0.037736</td>\n",
       "      <td>0.028302</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.526697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_titlecase_P</th>\n",
       "      <td>17</td>\n",
       "      <td>0.146226</td>\n",
       "      <td>0.146226</td>\n",
       "      <td>0.080189</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>10</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>0.535167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_titlecase_D</th>\n",
       "      <td>18</td>\n",
       "      <td>0.212264</td>\n",
       "      <td>0.212264</td>\n",
       "      <td>0.169811</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>13</td>\n",
       "      <td>0.288889</td>\n",
       "      <td>0.412569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_which</th>\n",
       "      <td>19</td>\n",
       "      <td>0.014151</td>\n",
       "      <td>0.014151</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.524031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_film</th>\n",
       "      <td>20</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.622605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_work</th>\n",
       "      <td>21</td>\n",
       "      <td>0.018868</td>\n",
       "      <td>0.018868</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.533173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_based</th>\n",
       "      <td>22</td>\n",
       "      <td>0.023585</td>\n",
       "      <td>0.023585</td>\n",
       "      <td>0.023585</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.525683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_quote_or_year</th>\n",
       "      <td>23</td>\n",
       "      <td>0.301887</td>\n",
       "      <td>0.301887</td>\n",
       "      <td>0.240566</td>\n",
       "      <td>45</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.703125</td>\n",
       "      <td>0.782424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_movie_direct_P</th>\n",
       "      <td>24</td>\n",
       "      <td>0.099057</td>\n",
       "      <td>0.099057</td>\n",
       "      <td>0.042453</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.555423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_genre_D</th>\n",
       "      <td>25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.535235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_genre_P</th>\n",
       "      <td>26</td>\n",
       "      <td>0.221698</td>\n",
       "      <td>0.221698</td>\n",
       "      <td>0.146226</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>30</td>\n",
       "      <td>0.638298</td>\n",
       "      <td>0.560012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_drama</th>\n",
       "      <td>27</td>\n",
       "      <td>0.028302</td>\n",
       "      <td>0.028302</td>\n",
       "      <td>0.028302</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.533452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_nominat</th>\n",
       "      <td>28</td>\n",
       "      <td>0.056604</td>\n",
       "      <td>0.056604</td>\n",
       "      <td>0.037736</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.536760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_final</th>\n",
       "      <td>29</td>\n",
       "      <td>0.014151</td>\n",
       "      <td>0.014151</td>\n",
       "      <td>0.014151</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.522338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_reunit</th>\n",
       "      <td>30</td>\n",
       "      <td>0.018868</td>\n",
       "      <td>0.018868</td>\n",
       "      <td>0.018868</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.537835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_document</th>\n",
       "      <td>31</td>\n",
       "      <td>0.018868</td>\n",
       "      <td>0.018868</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.617223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_first_capitalized_D</th>\n",
       "      <td>32</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.529287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_first_capitalized_P</th>\n",
       "      <td>33</td>\n",
       "      <td>0.466981</td>\n",
       "      <td>0.452830</td>\n",
       "      <td>0.367925</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "      <td>34</td>\n",
       "      <td>0.343434</td>\n",
       "      <td>0.500893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_names_between</th>\n",
       "      <td>34</td>\n",
       "      <td>0.367925</td>\n",
       "      <td>0.363208</td>\n",
       "      <td>0.330189</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>26</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.755467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_partners</th>\n",
       "      <td>35</td>\n",
       "      <td>0.037736</td>\n",
       "      <td>0.037736</td>\n",
       "      <td>0.028302</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.556621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_movie_between</th>\n",
       "      <td>36</td>\n",
       "      <td>0.047170</td>\n",
       "      <td>0.047170</td>\n",
       "      <td>0.047170</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.517085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_actors_P</th>\n",
       "      <td>37</td>\n",
       "      <td>0.014151</td>\n",
       "      <td>0.014151</td>\n",
       "      <td>0.004717</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.525041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_relations</th>\n",
       "      <td>38</td>\n",
       "      <td>0.070755</td>\n",
       "      <td>0.070755</td>\n",
       "      <td>0.070755</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.539685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LF_distant_supervision</th>\n",
       "      <td>39</td>\n",
       "      <td>0.051887</td>\n",
       "      <td>0.047170</td>\n",
       "      <td>0.042453</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.525226</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              j  Coverage  Overlaps  Conflicts  TP  FP  FN  \\\n",
       "LF_upper_lower_upper_P        0  0.047170  0.023585   0.023585  10   0   0   \n",
       "LF_year_P                     1  0.216981  0.216981   0.174528  30  16   0   \n",
       "LF_single_quotes_P            2  0.084906  0.084906   0.066038  15   3   0   \n",
       "LF_single_quotes_D            3  0.000000  0.000000   0.000000   0   0   0   \n",
       "LF_1_titlecase_apostrophe_D   4  0.099057  0.094340   0.089623  17   4   0   \n",
       "LF_1_titlecase_apostrophe_P   5  0.000000  0.000000   0.000000   0   0   0   \n",
       "LF_digit_P                    6  0.297170  0.297170   0.245283  47  16   0   \n",
       "LF_directed_by                7  0.325472  0.311321   0.297170  55  14   0   \n",
       "LF_parenthesis_right          8  0.136792  0.117925   0.108491  24   5   0   \n",
       "LF_year_between               9  0.193396  0.193396   0.174528   0   0  30   \n",
       "LF_star                      10  0.009434  0.009434   0.009434   0   0   2   \n",
       "LF_act                       11  0.047170  0.037736   0.014151   0   0   1   \n",
       "LF_with                      12  0.056604  0.056604   0.028302   0   0   3   \n",
       "LF_commas_between            13  0.207547  0.207547   0.202830   0   0  29   \n",
       "LF_feat                      14  0.056604  0.056604   0.037736   0   0   1   \n",
       "LF_featuring                 15  0.047170  0.047170   0.028302   0   0   0   \n",
       "LF_opposite                  16  0.037736  0.037736   0.028302   0   0   0   \n",
       "LF_titlecase_P               17  0.146226  0.146226   0.080189   0   0  21   \n",
       "LF_titlecase_D               18  0.212264  0.212264   0.169811   0   0  32   \n",
       "LF_which                     19  0.014151  0.014151   0.009434   0   0   0   \n",
       "LF_film                      20  0.009434  0.009434   0.000000   0   0   0   \n",
       "LF_work                      21  0.018868  0.018868   0.000000   0   0   0   \n",
       "LF_based                     22  0.023585  0.023585   0.023585   0   0   3   \n",
       "LF_quote_or_year             23  0.301887  0.301887   0.240566  45  19   0   \n",
       "LF_movie_direct_P            24  0.099057  0.099057   0.042453   0   0  11   \n",
       "LF_genre_D                   25  0.000000  0.000000   0.000000   0   0   0   \n",
       "LF_genre_P                   26  0.221698  0.221698   0.146226   0   0  17   \n",
       "LF_drama                     27  0.028302  0.028302   0.028302   0   0   2   \n",
       "LF_nominat                   28  0.056604  0.056604   0.037736   0   0   1   \n",
       "LF_final                     29  0.014151  0.014151   0.014151   0   0   0   \n",
       "LF_reunit                    30  0.018868  0.018868   0.018868   0   0   2   \n",
       "LF_document                  31  0.018868  0.018868   0.009434   0   0   1   \n",
       "LF_first_capitalized_D       32  0.009434  0.009434   0.000000   0   0   2   \n",
       "LF_first_capitalized_P       33  0.466981  0.452830   0.367925   0   0  65   \n",
       "LF_names_between             34  0.367925  0.363208   0.330189   0   0  52   \n",
       "LF_partners                  35  0.037736  0.037736   0.028302   0   0   0   \n",
       "LF_movie_between             36  0.047170  0.047170   0.047170   0   0   4   \n",
       "LF_actors_P                  37  0.014151  0.014151   0.004717   0   0   1   \n",
       "LF_relations                 38  0.070755  0.070755   0.070755   0   0   2   \n",
       "LF_distant_supervision       39  0.051887  0.047170   0.042453  11   0   0   \n",
       "\n",
       "                             TN  Empirical Acc.  Learned Acc.  \n",
       "LF_upper_lower_upper_P        0        1.000000      0.590970  \n",
       "LF_year_P                     0        0.652174      0.772611  \n",
       "LF_single_quotes_P            0        0.833333      0.541915  \n",
       "LF_single_quotes_D            0             NaN      0.526420  \n",
       "LF_1_titlecase_apostrophe_D   0        0.809524      0.537210  \n",
       "LF_1_titlecase_apostrophe_P   0             NaN      0.546671  \n",
       "LF_digit_P                    0        0.746032      0.764621  \n",
       "LF_directed_by                0        0.797101      0.538139  \n",
       "LF_parenthesis_right          0        0.827586      0.702266  \n",
       "LF_year_between              11        0.268293      0.481129  \n",
       "LF_star                       0        0.000000      0.522619  \n",
       "LF_act                        9        0.900000      0.553109  \n",
       "LF_with                       9        0.750000      0.551020  \n",
       "LF_commas_between            15        0.340909      0.743442  \n",
       "LF_feat                      11        0.916667      0.533938  \n",
       "LF_featuring                 10        1.000000      0.533858  \n",
       "LF_opposite                   8        1.000000      0.526697  \n",
       "LF_titlecase_P               10        0.322581      0.535167  \n",
       "LF_titlecase_D               13        0.288889      0.412569  \n",
       "LF_which                      3        1.000000      0.524031  \n",
       "LF_film                       2        1.000000      0.622605  \n",
       "LF_work                       4        1.000000      0.533173  \n",
       "LF_based                      2        0.400000      0.525683  \n",
       "LF_quote_or_year              0        0.703125      0.782424  \n",
       "LF_movie_direct_P            10        0.476190      0.555423  \n",
       "LF_genre_D                    0             NaN      0.535235  \n",
       "LF_genre_P                   30        0.638298      0.560012  \n",
       "LF_drama                      4        0.666667      0.533452  \n",
       "LF_nominat                   11        0.916667      0.536760  \n",
       "LF_final                      3        1.000000      0.522338  \n",
       "LF_reunit                     2        0.500000      0.537835  \n",
       "LF_document                   3        0.750000      0.617223  \n",
       "LF_first_capitalized_D        0        0.000000      0.529287  \n",
       "LF_first_capitalized_P       34        0.343434      0.500893  \n",
       "LF_names_between             26        0.333333      0.755467  \n",
       "LF_partners                   8        1.000000      0.556621  \n",
       "LF_movie_between              6        0.600000      0.517085  \n",
       "LF_actors_P                   2        0.666667      0.525041  \n",
       "LF_relations                 13        0.866667      0.539685  \n",
       "LF_distant_supervision        0        1.000000      0.525226  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Post-Train\n",
    "gen_model = GenerativeModel()\n",
    "gen_model.train(L_train, epochs=100, decay=0.95, step_size=0.1 / L_train.shape[0], reg_param=1e-6)\n",
    "print(\"LF weights:\", gen_model.weights.lf_accuracy)\n",
    "L_gold_dev = load_gold_labels(session, annotator_name='gold', split=1)\n",
    "L_dev = labeler.apply_existing(split=1)\n",
    "tp, fp, tn, fn = gen_model.error_analysis(session, L_dev, L_gold_dev)\n",
    "L_dev.lf_stats(session, L_gold_dev, gen_model.learned_lf_stats()['Accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check distribution of the training marginals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAANTElEQVR4nO3dfYxldX3H8fdHwNgWWjA7kA0PjjVoICYuZEJpSCyKmhUSwIQ2JZFiQ7vGSKMtabKxf0gf/tg+IEkTY10CgTaK0qqFCK0lBENthHYQxIWNAemWrmzYoYjSmNoC3/5xz7a7szN7z97H/S3vVzKZe889d+83vwzvPZy592yqCklSe1437wEkSaMx4JLUKAMuSY0y4JLUKAMuSY06dpYvtmHDhlpcXJzlS0pS8x5++OHnq2ph9faZBnxxcZHl5eVZvqQkNS/Jv6213VMoktQoAy5JjTLgktQoAy5JjTLgktQoAy5JjTLgktQoAy5JjTLgktSomX4SU5Jatbj17rGev2vbJROa5P95BC5JjTLgktQoAy5JjTLgktQoAy5JjTLgktQoAy5JjTLgktQoAy5JjTLgktQoAy5JjTLgktQoAy5JjTLgktSooQFP8oYk/5zk20keT/L73fY3J3koyZNJvpjk9dMfV5K0T58j8J8A766qdwCbgM1Jzgf+GLixqs4EfgBcM7UpJUkHGRrwGvjP7u5x3VcB7wb+ptt+G3D5NAaUJK2t1znwJMckeRTYC9wLfA94sape7nbZDZw6lQklSWvqFfCqeqWqNgGnAecBZ62121rPTbIlyXKS5ZWVlZEHlSQd6LDehVJVLwJfB84HTkyy79/UPA14dp3nbK+qpapaWlhYGGNUSdL++rwLZSHJid3tnwLeA+wE7geu6Ha7GrhzSjNKktbQ51+l3wjcluQYBsG/o6q+muQJ4AtJ/gh4BLh5inNKklYZGvCqegw4Z43tTzM4Hy5JmgM/iSlJjTLgktQoAy5JjTLgktQoAy5JjTLgktQoAy5JjTLgktQoAy5JjTLgktQoAy5JjTLgktQoAy5JjepzOdkjwuLWu0d+7q5tl0xwEkk6MngELkmNMuCS1CgDLkmNMuCS1CgDLkmNMuCS1CgDLkmNMuCS1CgDLkmNGhrwJKcnuT/JziSPJ/lYt/36JN9P8mj3dfH0x5Uk7dPno/QvA9dV1beSnAA8nOTe7rEbq+rPpjeeJGk9QwNeVXuAPd3tl5LsBE6d9mCSpEM7rHPgSRaBc4CHuk3XJnksyS1JTlrnOVuSLCdZXllZGW9aSdL/6R3wJMcDXwI+XlU/Aj4DvAXYxOAI/Ya1nldV26tqqaqWFhYWxp9YkgT0DHiS4xjE+3NV9WWAqnquql6pqleBm4DzpjemJGm1Pu9CCXAzsLOqPrXf9o377fYBYMfkx5MkrafPu1AuAK4CvpPk0W7bJ4Ark2wCCtgFfHgK80mS1tHnXSjfALLGQ/dMfhxJUl9+ElOSGmXAJalRBlySGmXAJalRBlySGmXAJalRBlySGmXAJalRBlySGmXAJalRBlySGmXAJalRBlySGmXAJalRBlySGmXAJalRBlySGmXAJalRBlySGmXAJalRBlySGmXAJalRQwOe5PQk9yfZmeTxJB/rtr8xyb1Jnuy+nzT9cSVJ+/Q5An8ZuK6qzgLOBz6a5GxgK3BfVZ0J3NfdlyTNyNCAV9WeqvpWd/slYCdwKnAZcFu3223A5VOaUZK0hsM6B55kETgHeAg4par2wCDywMnrPGdLkuUkyysrK2OOK0nap3fAkxwPfAn4eFX9qO/zqmp7VS1V1dLCwsIoM0qS1tAr4EmOYxDvz1XVl7vNzyXZ2D2+Edg7nRElSWvp8y6UADcDO6vqU/s9dBdwdXf7auDOyY8nSVrPsT32uQC4CvhOkke7bZ8AtgF3JLkGeAb45alMKEla09CAV9U3gKzz8EWTHUeS1JefxJSkRhlwSWqUAZekRhlwSWqUAZekRhlwSWqUAZekRhlwSWqUAZekRhlwSWqUAZekRhlwSWqUAZekRhlwSWqUAZekRhlwSWqUAZekRhlwSWqUAZekRhlwSWqUAZekRhlwSWqUAZekRg0NeJJbkuxNsmO/bdcn+X6SR7uvi6c7piRptT5H4LcCm9fYfmNVbeq+7pnsWJKkYYYGvKoeAF6YwSySpMMwzjnwa5M81p1iOWm9nZJsSbKcZHllZWWMl5Mk7W/UgH8GeAuwCdgD3LDejlW1vaqWqmppYWFhxJeTJK02UsCr6rmqeqWqXgVuAs6b7FiSpGFGCniSjfvd/QCwY719JUnTceywHZLcDlwIbEiyG/gkcGGSTUABu4APT29ESdJahga8qq5cY/PNU5hFknQY/CSmJDXKgEtSowy4JDXKgEtSowy4JDXKgEtSowy4JDXKgEtSowy4JDXKgEtSowy4JDXKgEtSowy4JDXKgEtSowy4JDXKgEtSowy4JDXKgEtSowy4JDVq6L+JqfEsbr175Ofu2nbJBCeRdLTxCFySGmXAJalRQwOe5JYke5Ps2G/bG5Pcm+TJ7vtJ0x1TkrRanyPwW4HNq7ZtBe6rqjOB+7r7kqQZGhrwqnoAeGHV5suA27rbtwGXT3YsSdIwo54DP6Wq9gB0309eb8ckW5IsJ1leWVkZ8eUkSatN/ZeYVbW9qpaqamlhYWHaLydJrxmjBvy5JBsBuu97JzeSJKmPUQN+F3B1d/tq4M7JjCNJ6qvP2whvB74JvC3J7iTXANuA9yZ5Enhvd1+SNENDP0pfVVeu89BFE55FknQY/CSmJDXKgEtSowy4JDXKgEtSowy4JDXKgEtSowy4JDXKgEtSowy4JDXKgEtSowy4JDXKgEtSowy4JDVq6NUIpVla3Hr3yM/dte2SCU4iHfk8ApekRhlwSWqUAZekRhlwSWqUAZekRhlwSWqUAZekRhlwSWrUWB/kSbILeAl4BXi5qpYmMZQkabhJfBLzXVX1/AT+HEnSYfAUiiQ1atwj8AL+IUkBn62q7at3SLIF2AJwxhlnjPly0nSMcw0W8Dosmo9xj8AvqKpzgfcDH03yztU7VNX2qlqqqqWFhYUxX06StM9YAa+qZ7vve4GvAOdNYihJ0nAjBzzJzyQ5Yd9t4H3AjkkNJkk6tHHOgZ8CfCXJvj/n81X19xOZSpI01MgBr6qngXdMcBZJ0mHwbYSS1CgDLkmNMuCS1CgDLkmNMuCS1CgDLkmNMuCS1KhJXE5WOsC4F4aS1I9H4JLUKAMuSY0y4JLUKAMuSY0y4JLUKAMuSY0y4JLUKAMuSY0y4JLUKAMuSY0y4JLUKAMuSY3yYlbSnI1z8a9d2y6Z4CRqjUfgktQoAy5JjRor4Ek2J/lukqeSbJ3UUJKk4UYOeJJjgE8D7wfOBq5McvakBpMkHdo4R+DnAU9V1dNV9d/AF4DLJjOWJGmYVNVoT0yuADZX1W90968CfqGqrl213xZgS3f3bcB3Rx937jYAz897iCOI63Ew1+RArseBRl2PN1XVwuqN47yNMGtsO+hvg6raDmwf43WOGEmWq2pp3nMcKVyPg7kmB3I9DjTp9RjnFMpu4PT97p8GPDveOJKkvsYJ+L8AZyZ5c5LXA78K3DWZsSRJw4x8CqWqXk5yLfA14Bjglqp6fGKTHZmOilNBE+R6HMw1OZDrcaCJrsfIv8SUJM2Xn8SUpEYZcElqlAFfw7BLBCT5nSRPJHksyX1J3jSPOWel7yUTklyRpJIc1W8b67MeSX6l+xl5PMnnZz3jrPX4b+aMJPcneaT77+biecw5C0luSbI3yY51Hk+SP+/W6rEk5478YlXl135fDH4h+z3g54HXA98Gzl61z7uAn+5ufwT44rznnud6dPudADwAPAgszXvuOf98nAk8ApzU3T953nMfAWuyHfhId/tsYNe8557ierwTOBfYsc7jFwN/x+CzNOcDD436Wh6BH2zoJQKq6v6q+nF390EG74E/WvW9ZMIfAn8C/Ncsh5uDPuvxm8Cnq+oHAFW1d8YzzlqfNSngZ7vbP8dR/JmRqnoAeOEQu1wG/GUNPAicmGTjKK9lwA92KvDv+93f3W1bzzUM/jY9Wg1djyTnAKdX1VdnOdic9Pn5eCvw1iT/lOTBJJtnNt189FmT64EPJtkN3AP81mxGOyIdbmPW5b/Ic7BelwgASPJBYAn4palONF+HXI8krwNuBD40q4HmrM/Px7EMTqNcyOD/zv4xydur6sXpjjY3fdbkSuDWqrohyS8Cf9WtyavTH++I07sxw3gEfrBelwhI8h7g94BLq+onM5ptHoatxwnA24GvJ9nF4JzeXUfxLzL7/HzsBu6sqv+pqn9lcAG3M2c03zz0WZNrgDsAquqbwBsYXNjptWhilyEx4AcbeomA7pTBZxnE+2g/v3nI9aiqH1bVhqparKpFBr8TuLSqlucz7tT1uYTE3zL4RTdJNjA4pfL0LIecsT5r8gxwEUCSsxgEfGWmUx457gJ+rXs3yvnAD6tqzyh/kKdQVql1LhGQ5A+A5aq6C/hT4Hjgr5MAPFNVl85t6CnquR6vGT3X42vA+5I8AbwC/G5V/cf8pp6unmtyHXBTkt9mcLrgQ9W9JeNok+R2BqfPNnTn/D8JHAdQVX/B4HcAFwNPAT8Gfn3k1zpK11CSjnqeQpGkRhlwSWqUAZekRhlwSWqUAZekRhlwSWqUAZekRv0vnDAgpmSxpt4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_marginals = gen_model.marginals(L_train)\n",
    "plt.hist(train_marginals, bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The distribution seems good, given that most classifiers are very close to either 0 or 1, as while it was initially very bad, but as I kept adding more labeling functions based on the FPs and FNs the distribution began to improve. The current distribution differentiates well between the classes, although its clear from the plot that defining what is a match is much easier than defining what is NOT a match."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training an Discriminative Model\n",
    "\n",
    "Using the noisy training labels we generated to train our end extraction model. In particular, we will be training a Bi-LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cands = session.query(performance_with_director).filter(performance_with_director.split == 0).order_by(performance_with_director.id).all()\n",
    "dev_cands   = session.query(performance_with_director).filter(performance_with_director.split == 1).order_by(performance_with_director.id).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.annotations import load_gold_labels\n",
    "\n",
    "L_gold_dev  = load_gold_labels(session, annotator_name='gold', split=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try tuning the hyper-parameters below to get your best F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LSTM] Training model\n",
      "[LSTM] n_train=82  #epochs=10  batch size=32\n",
      "[LSTM] Epoch 1 (0.39s)\tAverage loss=0.675067\tDev F1=14.37\n",
      "[LSTM] Epoch 2 (1.39s)\tAverage loss=0.570447\tDev F1=80.39\n",
      "[LSTM] Epoch 3 (2.31s)\tAverage loss=0.483079\tDev F1=81.98\n",
      "[LSTM] Epoch 4 (3.20s)\tAverage loss=0.438753\tDev F1=80.90\n",
      "[LSTM] Epoch 5 (4.08s)\tAverage loss=0.437775\tDev F1=81.89\n",
      "[LSTM] Epoch 6 (4.98s)\tAverage loss=0.434732\tDev F1=81.89\n",
      "[LSTM] Epoch 7 (5.93s)\tAverage loss=0.405896\tDev F1=82.29\n",
      "[LSTM] Epoch 8 (6.81s)\tAverage loss=0.444036\tDev F1=81.61\n",
      "[LSTM] Epoch 9 (7.72s)\tAverage loss=0.400482\tDev F1=81.27\n",
      "[LSTM] Model saved as <LSTM>\n",
      "[LSTM] Epoch 10 (8.64s)\tAverage loss=0.395008\tDev F1=81.98\n",
      "[LSTM] Model saved as <LSTM>\n",
      "[LSTM] Training done (9.20s)\n",
      "[LSTM] Loaded model <LSTM>\n"
     ]
    }
   ],
   "source": [
    "from snorkel.learning.pytorch import LSTM\n",
    "\n",
    "train_kwargs = {\n",
    "    'lr':            0.01, # learning rate of the model\n",
    "    'embedding_dim': 50,   # size of the feature vector\n",
    "    'hidden_dim':    20,   # number of nodes in each layer in the model\n",
    "    'n_epochs':      10,   # number of training epochs\n",
    "    'dropout':       0.5,  # dropout rate (during learning)\n",
    "    'batch_size':    32,   # training batch size\n",
    "    'seed':          1701\n",
    "}\n",
    "\n",
    "lstm = LSTM(n_threads=-1)\n",
    "lstm.train(train_cands, train_marginals, X_dev=dev_cands, Y_dev=L_gold_dev, **train_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper-Parameter Tuning\n",
    "\n",
    "Surprisingly, I found that 50 neurons in the hidden layer were actually too much for this ultra-small (82 samples) dataset, and when I cut the neurons to 20 I saw a good increase in F1 score (mostly because with more neurons recall starts to worsen).\n",
    "\n",
    "I reduced the learning rate by an order of magnitudef which improved performance too. Presumably because given such a small, overfit-prone dataset, we really have to go slow then training.\n",
    "\n",
    "Given the small train dataset size, one of the most significative changes I made was reducing the batch sizea from 64 to 32, which seems to have made overfitting slightly less of an issue.\n",
    "\n",
    "Lastly, I've increased the Dropout rate from 0.2 to 0.33 as that showed improvements (further increases didn't)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report Performance of the Final Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prec: 0.716, Recall: 0.959, F1 Score: 0.820\n"
     ]
    }
   ],
   "source": [
    "p, r, f1 = lstm.score(dev_cands, L_gold_dev)\n",
    "print(\"Prec: {0:.3f}, Recall: {1:.3f}, F1 Score: {2:.3f}\".format(p, r, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It took a lot of tweaking, but in the end I managed to convert the good distribution of marginals into good F1 Scores with the trained model. This was mostly a results of tuning hyperparameters with the small dataset in mind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Scores (Un-adjusted)\n",
      "========================================\n",
      "Pos. class accuracy: 0.959\n",
      "Neg. class accuracy: 0.138\n",
      "Precision            0.716\n",
      "Recall               0.959\n",
      "F1                   0.82\n",
      "----------------------------------------\n",
      "TP: 141 | FP: 56 | TN: 9 | FN: 6\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tp, fp, tn, fn = lstm.error_analysis(session, dev_cands, L_gold_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally speaking my model's weak point is in accurately predicting non-matches, as defining what is _NOT_ a match in a string proved to be the hardest aspect of developing the labeling functions, as can be seen by the difficulty of getting a bar stacked at 0 on the marginals.\n",
    "\n",
    "The low accuracy for the negative class is a result of too many false positieves, which I was forced to content which precisely because of the difficulty of defining a non-match. If I use more relaxed labelings for that, then the model predicts nothing in the negative class, which makes for worse results, and thus I opted for a model with errs on the side of predicitng negative, which was the least of the evils."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the new model to extract relation in testing documents, and save it to JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions_df.shape: (212, 4)\n"
     ]
    }
   ],
   "source": [
    "# Export the DEV set prediction to a CSV file\n",
    "\n",
    "list_performances = []\n",
    "list_directors = []\n",
    "list_ids = []\n",
    "for i in range(len(dev_cands)):\n",
    "    list_ids.append(dev_cands[i][0].get_stable_id().split('::')[0])\n",
    "    list_performances.append(dev_cands[i][0].get_attrib_span('words'))\n",
    "    list_directors.append(dev_cands[i][1].get_attrib_span('words'))\n",
    "    \n",
    "\n",
    "dev_preds = lstm.predictions(dev_cands)\n",
    "\n",
    "predictions_df = pd.DataFrame(data = {'id': list_ids,\n",
    "                                      'performance': list_performances,\n",
    "                                      'director': list_directors,\n",
    "                                      'prediction': dev_preds})\n",
    "print(f'predictions_df.shape: {predictions_df.shape}')\n",
    "predictions_df.to_csv(HW_DIR / \"Matheus_Schmitz_hw05_pred.dev.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End\n",
    "Matheus Schmitz\n",
    "<br><a href=\"https://www.linkedin.com/in/matheusschmitz/\">LinkedIn</a></br>\n",
    "<br><a href=\"https://matheus-schmitz.github.io/\">Github Portfolio</a></br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "nteract": {
   "version": "0.28.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
