{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Snorkel to Extract Performances and their Directors\n",
    "\n",
    "**Notes**: \n",
    "- You are supposed to write your code or modify our code in any cell with `# TODO`.\n",
    "- Much content of this notebook was borrowed from Snorkel Introduction Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State-of-the-art extraction techniques require massive labeled training set but it is costly to obtain. To overcome this problem, Snorkel helps rapidly create training sets using the new data programming paradigm. To start, developers focus on writing a set of labeling functions, which are just scripts that programmatically label data. The resulting labels are noisy, but Snorkel uses a generative model to learn how to use those labeling functions to label more data. The new labeled data now can be used to train high-quality end models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In summary, in this task, you will first manually label 50 documents and use these labeled data as a development set to create your own labeling functions. Then, you will train a generative model to label the rest 450 documents in training set. Finally, you will train a discriminative model (Bi-LSTM) to produce your final extraction model!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "\n",
    "**In this homework, you need to extract the list of `performances` and their `directors` from the set of IMDB biographies that you collect for Homework 2. For example, you need to extract three tuples: [(`Lost on Purpose`, `the Nelms Brothers`), (`Waffle Street`, `the Nelms Brothers`), (`Small Town Crime`, `the Nelms Brothers`)] from the following sentence.**\n",
    "\n",
    "```\n",
    "He would go on to act in three consecutive, but very different films written and directed by the Nelms Brothers: Lost on Purpose, Waffle Street and Small Town Crime.\n",
    "```\n",
    "\n",
    "**In cases where your collected biographies do not contain enough pairs of `performances` and `directors`, please feel free to use the example dataset as well**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: COMBINE ALL OF YOUR BIOGRAPHIES IN ONE CSV FILE AND SUBMIT \"Firstname_Lastname_hw05_all.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare environment\n",
    "\n",
    "Lets install the packages we will use. Through my testing, Snorkel v0.7 works the best with Python 3.6 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are using Anaconda, you can create a new Python 3.6 environment.\n",
    "\n",
    "# !conda create -n py36 python=3.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work with Snorkel version 0.7 (Beta), we can retrieve it by running the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!curl -L \"https://github.com/snorkel-team/snorkel/archive/v0.7.0-beta.tar.gz\" -o snorkel_v0_7_0.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's uncompress the package and install Snorkel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!tar -xvzf snorkel_v0_7_0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install snorkel-0.7.0-beta/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a development set\n",
    "\n",
    "We need to preprocess our documents using `Snorkel` utilities, parsing them into a simple hierarchy of component parts of our input data, which we refer as _contexts_. We'll also create _candidates_ out of these contexts, which are the objects we want to classify, in this case, possible mentions of schools and colleges that the cast have attended. Finally, we'll load some gold labels for evaluation.\n",
    "\n",
    "All of this preprocessed input data is saved to a database. In Snorkel, if no database is specified, then a SQLite database at `./snorkel.db` is created by default -- so no setup is needed here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** STUDENT CODE\n",
    "\n",
    "import numpy as np, os\n",
    "from pathlib import Path\n",
    "\n",
    "from snorkel import SnorkelSession\n",
    "from snorkel.parser import TSVDocPreprocessor, CorpusParser\n",
    "from snorkel.parser.spacy_parser import Spacy\n",
    "from snorkel.models import Document, Sentence, candidate_subclass\n",
    "from snorkel.viewer import SentenceNgramViewer\n",
    "from snorkel.annotations import LabelAnnotator, load_gold_labels\n",
    "\n",
    "# TODO: SET LOCATION WHERE YOU STORE YOUR HW5 FILES\n",
    "if 'HW_DIR' not in os.environ:\n",
    "    HW_DIR = Path(\".\")\n",
    "else:\n",
    "    HW_DIR = Path(os.environ['HW_DIR'])\n",
    "    assert HW_DIR.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing a `SnorkelSession`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "session = SnorkelSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Corpus\n",
    "\n",
    "Next, we load and pre-process the corpus of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_preprocessor = TSVDocPreprocessor(HW_DIR / 'cast_bios.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a `CorpusParser`\n",
    "\n",
    "We'll use [Spacy](https://spacy.io/), an NLP preprocessing tool, to split our documents into sentences and tokens, and provide named entity annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "Wall time: 1min 16s\n"
     ]
    }
   ],
   "source": [
    "# Uncomment this to download spacy model\n",
    "# !python -m spacy download [model_name] (e.g. en_core_web_lg)\n",
    "\n",
    "corpus_parser = CorpusParser(parser=Spacy())\n",
    "%time corpus_parser.apply(doc_preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use simple database queries (written in the syntax of [SQLAlchemy](http://www.sqlalchemy.org/), which Snorkel uses) to check how many documents and sentences were parsed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents: 1423\n",
      "Sentences: 8137\n"
     ]
    }
   ],
   "source": [
    "print(\"Documents:\", session.query(Document).count())\n",
    "print(\"Sentences:\", session.query(Sentence).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Candidates\n",
    "\n",
    "The next step is to extract _candidates_ from our corpus. A `Candidate` in Snorkel is an object for which we want to make a prediction. In this case, the candidates are pairs of performances and directors mentioned in sentences.\n",
    "\n",
    "The [Spacy](https://spacy.io/) parser we used performs _named entity recognition_ for us. Next, we'll split up the documents into train and development splits; and collect the associated sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing a simple director name matcher\n",
    "\n",
    "Our **simple** name matcher makes use of the fact that the names of the directors are mentions of person-type named entities in the documents. `Fonduer` provides a list of built-in matchers that can be used in many information extraction tasks. We will use `PersonMatcher` to extract director names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.matchers import PersonMatcher, OrganizationMatcher\n",
    "\n",
    "director_matcher = PersonMatcher(longest_match_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** STUDENT CODE\n",
    "\n",
    "# TODO: WRITE YOUR PERFORMANCE MATCHER. YOU CAN REUSE EXTRACTORS IN HOMEWORK 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing a random performance matcher\n",
    "\n",
    "We design our **random** award matcher to capture all capitalized `span`s of text that contain the letter `A`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.matchers import RegexMatchEach, LambdaFunctionMatcher\n",
    "\n",
    "def mention_span_captilized_with_A(mention):\n",
    "    performance_string = mention.get_span()\n",
    "    for word in performance_string.split():\n",
    "        if word[0].islower():\n",
    "            return False\n",
    "    if \"A\" in performance_string:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "performance_matcher = LambdaFunctionMatcher(func=mention_span_captilized_with_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** STUDENT CODE\n",
    "\n",
    "# TODO: WRITE YOUR DIRECTOR MATCHER. YOU CAN REUSE EXTRACTORS IN HOMEWORK 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that normally each `director` name will contain at least two words (first name, last name). Considering\n",
    "additional middle names, we expect a maximum of four words per name.\n",
    "\n",
    "Similarly, we assume the `performance` name to be a `span` of one to seven words.\n",
    "\n",
    "We use the default `Ngrams` class provided by `Fonduer` to define these properties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.candidates import Ngrams\n",
    "# ** STUDENT CODE\n",
    "\n",
    "# TODO: FEEL FREE TO CHANGE THE NGRAMS LENGTH IF YOU WANT\n",
    "performance_ngrams = Ngrams(n_max=4)\n",
    "director_ngrams = Ngrams(n_max=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a candidate that is composed of a `performance` and a `director` mention as we defined above. We name this candidate `performance_director`. And we will extract all "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.candidates import Ngrams, CandidateExtractor\n",
    "\n",
    "performance_with_director = candidate_subclass('performance_director', ['performance', 'director'])\n",
    "ngrams = Ngrams(n_max=7)\n",
    "cand_extractor = CandidateExtractor(performance_with_director, [performance_ngrams, director_ngrams], [director_matcher, performance_matcher])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the development set\n",
    "\n",
    "We create our development set by generating a `dev_ids.csv` file, which has one column `id` and contains 50 random biography URLs. You can choose any subset of 50 biographies that have `performance` and `director`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dev documents: 1\n",
      "Number of dev sents: 7\n",
      "Number of train sents: 8130\n"
     ]
    }
   ],
   "source": [
    "docs = session.query(Document).order_by(Document.name).all()\n",
    "import pandas as pd\n",
    "\n",
    "docs = session.query(Document).order_by(Document.name).all()\n",
    "ld = len(docs)\n",
    "\n",
    "gold_data = pd.read_csv(\"dev_ids.csv\")\n",
    "\n",
    "dev_docs = gold_data[\"id\"].values.tolist()\n",
    "\n",
    "print(f\"Number of dev documents: {len(dev_docs)}\")\n",
    "\n",
    "train_sents = set()\n",
    "dev_sents   = set()\n",
    "\n",
    "for doc in docs:\n",
    "    sents = [s for s in doc.sentences]\n",
    "    if doc.name in dev_docs:\n",
    "        dev_sents.update(sents)\n",
    "    else:\n",
    "        train_sents.update(sents)\n",
    "        \n",
    "print(\"Number of dev sents:\", len(dev_sents))\n",
    "print(\"Number of train sents:\", len(train_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll apply the candidate extractor to the two sets of sentences. The results will be persisted in the database backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "Number of candidates: 4697\n",
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "\n",
      "Number of candidates: 5\n",
      "Wall time: 27.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i, sents in enumerate([train_sents, dev_sents]):\n",
    "    cand_extractor.apply(sents, split=i)\n",
    "    print(\"Number of candidates:\", session.query(performance_with_director).filter(performance_with_director.split == i).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label 50 documents in development set\n",
    "\n",
    "In this task, you will use `SentenceNgramViewer` to label each mention. You can click the green button to mark the candidate as correct, red button to mark as incorrect. Your labeling result is automatically stored in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.models import GoldLabel, GoldLabelKey\n",
    "\n",
    "def get_gold_labels(session: SnorkelSession, annotator_name: str=\"gold\"):\n",
    "    # define relationship in case it is not defined\n",
    "    ak = session.query(GoldLabelKey).filter(GoldLabelKey.name == annotator_name).first()\n",
    "    return session.query(GoldLabel).filter(GoldLabel.key == ak).all()\n",
    "\n",
    "gold_labels = get_gold_labels(session)\n",
    "labeled_sents = {lbl.candidate.performance.sentence.id for lbl in gold_labels}\n",
    "unlabeled = [\n",
    "    x for x in session.query(performance_with_director).filter(performance_with_director.split == 1).all() \n",
    "    if x.performance.sentence.id not in labeled_sents\n",
    "]\n",
    "print(\"Number unlabeled:\", len(unlabeled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please remember to label all pairs of mentions, both correct and incorrect ones**\n",
    "\n",
    "`SentenceNgramViewer` only show candidates that are matched by your matchers. Therefore, your annotation is under an assumption that your matchers work perfectly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run this if you see \"SentenceNgramViewer\" text instead of a UI component. Then restart your notebook and refresh your browser.\n",
    "\n",
    "#!jupyter nbextension enable --py --sys-prefix widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SentenceNgramViewer(unlabeled, session, annotator_name=\"gold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you finish labeling, executing the cell below to **save your result** to CSV files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** STUDENT CODE\n",
    "\n",
    "def extract_gold_labels(session: SnorkelSession, annotator_name: str=\"gold\", split: int=None):\n",
    "    ''' Extract pairwise gold labels and store in a file. '''\n",
    "    gold_labels = get_gold_labels(session, annotator_name)\n",
    "\n",
    "    results = []\n",
    "    for gold_label in gold_labels:\n",
    "        rel = gold_label.candidate\n",
    "        if split is not None and rel.split != split:\n",
    "            continue\n",
    "\n",
    "        results.append({\n",
    "            \"id\": rel.performance.sentence.document.name,\n",
    "            \"performance\": rel.performance.get_span(),\n",
    "            \"director\": rel.director.get_span(),\n",
    "            \"value\": gold_label.value\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "gold_labels = extract_gold_labels(session, split=1)\n",
    "gold_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: CHANGE TO YOUR NAME AND SAVE THE GOLD LABELS (TASK 1)\n",
    "pd.DataFrame(gold_labels).to_csv(\"Firstname_Lastname_hw05_gold.dev.csv\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define labeling functions (LFs)\n",
    "\n",
    "In this task, you will define your own LFs, which Snorkel uses to create noise-aware training set. Usually, you will go through a couple of iterations (create LFs, test and refine it) to come up with a good set of LFs. We provide you at the end of this section a helper to quickly see what candidates did your model fail to classify. You can refer to [Snorkel tutorial](https://github.com/snorkel-team/snorkel-extraction/tree/master/tutorials) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are free to use write any extra code to create a set of sophisticated LFs. More LF helper functions can be found [here](https://github.com/snorkel-team/snorkel-extraction/blob/master/snorkel/lf_helpers.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** STUDENT CODE \n",
    "\n",
    "# THESE ARE SOME HELPER FUNCTIONS THAT YOU CAN USE\n",
    "from snorkel.lf_helpers import (\n",
    "    get_left_tokens, get_right_tokens, get_between_tokens,\n",
    "    get_text_between, get_tagged_text,\n",
    ")\n",
    "\n",
    "# TODO: DEFINE YOUR LFS HERE. BELOW ARE SOME RANDOM LFS\n",
    "\n",
    "ABSTAIN = -1\n",
    "FALSE = 0\n",
    "TRUE = 1\n",
    "\n",
    "\n",
    "def random_lf1(c):\n",
    "    p1 = c.performance.get_word_start()\n",
    "    p2 = c.director.get_word_start()\n",
    "    if p1 < p2:\n",
    "        return TRUE\n",
    "    else:\n",
    "        return FALSE\n",
    "    \n",
    "def random_lf2(c):\n",
    "    p1 = c.performance.get_word_start()\n",
    "    p2 = c.director.get_word_start()\n",
    "    if p1 > p2:\n",
    "        return TRUE\n",
    "    else:\n",
    "        return FALSE\n",
    "    \n",
    "def random_lf3(c):\n",
    "    p1 = c.performance.get_word_start()\n",
    "    p2 = c.director.get_word_start()\n",
    "    if p1 == p2:\n",
    "        return TRUE\n",
    "    else:\n",
    "        return FALSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** STUDENT CODE\n",
    "\n",
    "# TODO: PUT ALL YOUR LABELING FUNCTIONS HERE\n",
    "\n",
    "performance_with_director_lfs = [\n",
    "    random_lf1,\n",
    "    random_lf2,\n",
    "    random_lf3\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train generative model\n",
    "\n",
    "Now, we'll train a model of the LFs to estimate their accuracies. Once the model is trained, we can combine the outputs of the LFs into a single, noise-aware training label set for our extractor. Intuitively, we'll model the LFs by observing how they overlap and conflict with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1701)\n",
    "\n",
    "labeler = LabelAnnotator(lfs=performance_with_director_lfs)\n",
    "L_train = labeler.apply(split=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get detailed statistics of LFs before training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_train.lf_stats(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: MAKE SURE THE ABOVE CELL OUTPUT IS SHOWN IN YOUR PDF VERSION. THIS WILL BE YOUR ANSWER FOR TASK 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.learning import GenerativeModel\n",
    "\n",
    "gen_model = GenerativeModel()\n",
    "gen_model.train(L_train, epochs=100, decay=0.95, step_size=0.1 / L_train.shape[0], reg_param=1e-6)\n",
    "\n",
    "print(\"LF weights:\", gen_model.weights.lf_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: MAKE SURE THE ABOVE CELL OUTPUT IS SHOWN IN YOUR PDF VERSION. THIS WILL BE YOUR ANSWER FOR TASK 2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have learned the generative model, we will measure its performances using the provided test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_gold_dev = load_gold_labels(session, annotator_name='gold', split=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_dev = labeler.apply_existing(split=1)\n",
    "tp, fp, tn, fn = gen_model.error_analysis(session, L_dev, L_gold_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get detailed statistics of LFs learned by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_dev.lf_stats(session, L_gold_dev, gen_model.learned_lf_stats()['Accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: MAKE SURE THE ABOVE CELL OUTPUT IS SHOWN IN YOUR PDF VERSION. THIS WILL BE YOUR ANSWER FOR TASK 2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now apply the generative model to the training candidates to get the noise-aware training label set. We'll refer to these as the training marginals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_marginals = gen_model.marginals(L_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll look at the distribution of the training marginals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(train_marginals, bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: MAKE SURE THE ABOVE CELL OUTPUT IS SHOWN IN YOUR PDF VERSION. THIS WILL BE YOUR ANSWER FOR TASK 2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: CHANGE THIS CELL TO MARKDOWN CELL AND WRITE YOUR ANSWER TO TASK 2.5 HERE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might want to look at some examples in one of the error buckets to improve your LFs. For example, below is one of the false positives that we did not correctly label correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SentenceNgramViewer(fp, session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Distant Supervision Labeling Function\n",
    "\n",
    "Distant supervision generates training data automatically using an external, imperfectly aligned training resource, such as a Knowledge Base.\n",
    "\n",
    "Define an additional distant-supervision-based labeling function which uses Wikidata or DBpedia. With the additional labeling function you added, please make sure to answer all questions for Task 3.3, 3.4, 3.5 mentioned in the homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: ADD YOUR DISTANT SUPERVISION LABELING FUNCTIONS AND ANSWER TASK 3 QUESTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training an Discriminative Model\n",
    "\n",
    "In this final task, we'll use the noisy training labels we generated to train our end extraction model. In particular, we will be training a Bi-LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cands = session.query(performance_with_director).filter(performance_with_director.split == 0).order_by(performance_with_director.id).all()\n",
    "dev_cands   = session.query(performance_with_director).filter(performance_with_director.split == 1).order_by(performance_with_director.id).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.annotations import load_gold_labels\n",
    "\n",
    "L_gold_dev  = load_gold_labels(session, annotator_name='gold', split=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try tuning the hyper-parameters below to get your best F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** STUDENT CODE\n",
    "\n",
    "# TODO: TUNE YOUR HYPERPARAMETERS TO OBTAIN BEST RESULTS. WE EXPECT A F1-SCORE THAT IS HIGHER THAN 0.7\n",
    "\n",
    "from snorkel.learning.pytorch import LSTM\n",
    "\n",
    "train_kwargs = {\n",
    "    'lr':            0.01, # learning rate of the model\n",
    "    'embedding_dim': 50,   # size of the feature vector\n",
    "    'hidden_dim':    50,   # number of nodes in each layer in the model\n",
    "    'n_epochs':      10,   # number of training epochs\n",
    "    'dropout':       0.2,  # dropout rate (during learning)\n",
    "    'batch_size':    64,   # training batch size\n",
    "    'seed':          1701\n",
    "}\n",
    "\n",
    "lstm = LSTM(n_threads=None)\n",
    "lstm.train(train_cands, train_marginals, X_dev=dev_cands, Y_dev=L_gold_dev, **train_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report performance of your final extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, r, f1 = lstm.score(dev_cands, L_gold_dev)\n",
    "print(\"Prec: {0:.3f}, Recall: {1:.3f}, F1 Score: {2:.3f}\".format(p, r, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: MAKE SURE THE ABOVE CELL OUTPUT IS SHOWN IN YOUR PDF VERSION. THIS WILL BE YOUR ANSWER FOR TASK 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp, fp, tn, fn = lstm.error_analysis(session, dev_cands, L_gold_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: MAKE SURE THE ABOVE CELL OUTPUT IS SHOWN IN YOUR PDF VERSION. THIS WILL BE YOUR ANSWER FOR TASK 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use your new model to extract relation in testing documents, and save it to JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** STUDENT CODE\n",
    "\n",
    "# TODO: EXPORT YOUR PREDICTION OF THE DEV SET TO A CSV FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "nteract": {
   "version": "0.28.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
